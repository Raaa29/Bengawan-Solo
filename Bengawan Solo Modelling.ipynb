{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ZJ4wROhtXGaX",
        "outputId": "71e7017e-3603-4ed4-bcca-d473b2c938a6"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Trial 15 Complete [00h 00m 47s]\n",
            "val_mae: 0.16359244287014008\n",
            "\n",
            "Best val_mae So Far: 0.1254303753376007\n",
            "Total elapsed time: 00h 28m 35s\n",
            "\n",
            "        Hyperparameter terbaik (V2 Final) yang ditemukan:\n",
            "        Embedding Dim: 48\n",
            "        MLP Unit 1: 48\n",
            "        MLP Unit 2: 16\n",
            "        MLP Unit 3: 16\n",
            "        Feature MLP Unit 1: 32\n",
            "        Feature MLP Unit 2: 24\n",
            "        Dropout Rate: 0.350\n",
            "        Learning Rate: 0.0005\n",
            "        L2 Reg Factor: 1e-05\n",
            "        \n",
            "\n",
            "Melatih model terbaik (V2 Final) dengan hyperparameter yang ditemukan...\n",
            "Epoch 1/120\n",
            "\u001b[1m511/511\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m13s\u001b[0m 12ms/step - loss: 0.2002 - mae: 0.3699 - val_loss: 0.0572 - val_mae: 0.1758\n",
            "Epoch 2/120\n",
            "\u001b[1m511/511\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 5ms/step - loss: 0.0590 - mae: 0.1659 - val_loss: 0.0553 - val_mae: 0.1562\n",
            "Epoch 3/120\n",
            "\u001b[1m511/511\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 4ms/step - loss: 0.0545 - mae: 0.1521 - val_loss: 0.0568 - val_mae: 0.1517\n",
            "Epoch 4/120\n",
            "\u001b[1m511/511\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 4ms/step - loss: 0.0449 - mae: 0.1262 - val_loss: 0.0595 - val_mae: 0.1479\n",
            "Epoch 5/120\n",
            "\u001b[1m511/511\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 4ms/step - loss: 0.0350 - mae: 0.1007 - val_loss: 0.0611 - val_mae: 0.1416\n",
            "Epoch 6/120\n",
            "\u001b[1m511/511\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 4ms/step - loss: 0.0285 - mae: 0.0855 - val_loss: 0.0622 - val_mae: 0.1386\n",
            "Epoch 7/120\n",
            "\u001b[1m511/511\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 4ms/step - loss: 0.0243 - mae: 0.0777 - val_loss: 0.0615 - val_mae: 0.1396\n",
            "Epoch 8/120\n",
            "\u001b[1m511/511\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 5ms/step - loss: 0.0213 - mae: 0.0700 - val_loss: 0.0618 - val_mae: 0.1395\n",
            "Epoch 9/120\n",
            "\u001b[1m511/511\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 4ms/step - loss: 0.0185 - mae: 0.0608 - val_loss: 0.0624 - val_mae: 0.1377\n",
            "Epoch 10/120\n",
            "\u001b[1m511/511\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 4ms/step - loss: 0.0165 - mae: 0.0541 - val_loss: 0.0629 - val_mae: 0.1355\n",
            "Epoch 11/120\n",
            "\u001b[1m511/511\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 4ms/step - loss: 0.0150 - mae: 0.0489 - val_loss: 0.0634 - val_mae: 0.1341\n",
            "Epoch 12/120\n",
            "\u001b[1m511/511\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 5ms/step - loss: 0.0141 - mae: 0.0454 - val_loss: 0.0650 - val_mae: 0.1320\n",
            "Epoch 13/120\n",
            "\u001b[1m511/511\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 5ms/step - loss: 0.0134 - mae: 0.0429 - val_loss: 0.0647 - val_mae: 0.1297\n",
            "Epoch 14/120\n",
            "\u001b[1m511/511\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 4ms/step - loss: 0.0130 - mae: 0.0411 - val_loss: 0.0656 - val_mae: 0.1292\n",
            "Epoch 15/120\n",
            "\u001b[1m511/511\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 4ms/step - loss: 0.0125 - mae: 0.0398 - val_loss: 0.0653 - val_mae: 0.1283\n",
            "Epoch 16/120\n",
            "\u001b[1m511/511\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 4ms/step - loss: 0.0120 - mae: 0.0377 - val_loss: 0.0658 - val_mae: 0.1304\n",
            "Epoch 17/120\n",
            "\u001b[1m511/511\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 5ms/step - loss: 0.0117 - mae: 0.0367 - val_loss: 0.0663 - val_mae: 0.1291\n",
            "Epoch 18/120\n",
            "\u001b[1m511/511\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 5ms/step - loss: 0.0119 - mae: 0.0368 - val_loss: 0.0668 - val_mae: 0.1301\n",
            "Epoch 19/120\n",
            "\u001b[1m511/511\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 4ms/step - loss: 0.0117 - mae: 0.0360 - val_loss: 0.0668 - val_mae: 0.1276\n",
            "Epoch 20/120\n",
            "\u001b[1m511/511\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 4ms/step - loss: 0.0115 - mae: 0.0349 - val_loss: 0.0680 - val_mae: 0.1291\n",
            "Epoch 21/120\n",
            "\u001b[1m511/511\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 4ms/step - loss: 0.0115 - mae: 0.0344 - val_loss: 0.0679 - val_mae: 0.1274\n",
            "Epoch 22/120\n",
            "\u001b[1m511/511\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 4ms/step - loss: 0.0113 - mae: 0.0343 - val_loss: 0.0685 - val_mae: 0.1283\n",
            "Epoch 23/120\n",
            "\u001b[1m511/511\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 5ms/step - loss: 0.0110 - mae: 0.0333 - val_loss: 0.0685 - val_mae: 0.1279\n",
            "Epoch 24/120\n",
            "\u001b[1m511/511\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 4ms/step - loss: 0.0108 - mae: 0.0321 - val_loss: 0.0677 - val_mae: 0.1291\n",
            "Epoch 25/120\n",
            "\u001b[1m511/511\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 4ms/step - loss: 0.0107 - mae: 0.0321 - val_loss: 0.0675 - val_mae: 0.1295\n",
            "Epoch 26/120\n",
            "\u001b[1m511/511\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 4ms/step - loss: 0.0105 - mae: 0.0313 - val_loss: 0.0682 - val_mae: 0.1288\n",
            "Epoch 27/120\n",
            "\u001b[1m511/511\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 4ms/step - loss: 0.0103 - mae: 0.0302 - val_loss: 0.0680 - val_mae: 0.1289\n",
            "Epoch 28/120\n",
            "\u001b[1m511/511\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 5ms/step - loss: 0.0101 - mae: 0.0302 - val_loss: 0.0665 - val_mae: 0.1299\n",
            "Epoch 29/120\n",
            "\u001b[1m511/511\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 4ms/step - loss: 0.0101 - mae: 0.0300 - val_loss: 0.0677 - val_mae: 0.1293\n",
            "Epoch 30/120\n",
            "\u001b[1m511/511\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 4ms/step - loss: 0.0101 - mae: 0.0300 - val_loss: 0.0675 - val_mae: 0.1288\n",
            "Epoch 31/120\n",
            "\u001b[1m511/511\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 5ms/step - loss: 0.0099 - mae: 0.0295 - val_loss: 0.0677 - val_mae: 0.1297\n",
            "Epoch 32/120\n",
            "\u001b[1m511/511\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 5ms/step - loss: 0.0095 - mae: 0.0284 - val_loss: 0.0671 - val_mae: 0.1296\n",
            "Epoch 33/120\n",
            "\u001b[1m511/511\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 4ms/step - loss: 0.0092 - mae: 0.0278 - val_loss: 0.0678 - val_mae: 0.1301\n",
            "Epoch 34/120\n",
            "\u001b[1m511/511\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 4ms/step - loss: 0.0091 - mae: 0.0273 - val_loss: 0.0671 - val_mae: 0.1282\n",
            "Epoch 35/120\n",
            "\u001b[1m511/511\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 4ms/step - loss: 0.0092 - mae: 0.0274 - val_loss: 0.0672 - val_mae: 0.1285\n",
            "Epoch 36/120\n",
            "\u001b[1m511/511\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 4ms/step - loss: 0.0090 - mae: 0.0274 - val_loss: 0.0672 - val_mae: 0.1288\n",
            "Epoch 36: early stopping\n",
            "Restoring model weights from the end of the best epoch: 21.\n",
            "\n",
            "Model Tuned (V2 Final) - Test Loss: 0.0679\n",
            "Model Tuned (V2 Final) - Test MAE: 0.1274\n",
            "\n",
            ">>> SKENARIO 1 (V2 Final): Rekomendasi untuk pengguna dikenal 'U000001'\n",
            "Membuat rekomendasi untuk pengguna yang sudah dikenal: U000001\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "<ipython-input-1-96ca21ba768e>:461: SettingWithCopyWarning: \n",
            "A value is trying to be set on a copy of a slice from a DataFrame.\n",
            "Try using .loc[row_indexer,col_indexer] = value instead\n",
            "\n",
            "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
            "  top_n_final['ID Tempat'] = top_n_final['item_id_int'].map(item_id_int_to_raw_map)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Top 5 Rekomendasi Gabungan (NCF + Cosine Boost) untuk User 'U000001':\n",
            "  ID Tempat                                     Nama Wisata  ncf_score_norm  content_affinity_score  combined_score %\n",
            "0      T020                              Pura Mangkunagaran        0.998111                0.053349         71.468259\n",
            "1      T044                          Rumah Atsiri Indonesia        0.999681                0.045276         71.335913\n",
            "2      T016  Museum Manusia Purba Sangiran Klaster Krikilan        0.999228                0.041849         71.201419\n",
            "3      T009                               Taman Balekambang        0.999547                0.040549         71.184781\n",
            "4      T043                                     Candi Sukuh        0.999989                0.038272         71.147385\n",
            "\n",
            ">>> SKENARIO 2 (V2 Final): Rekomendasi untuk pengguna baru, preferensi: ['budaya', 'seni'] (dengan NCF Probe)\n",
            "Probe User IDs: ['U001505', 'U015015', 'U034974']\n",
            "Membuat rekomendasi untuk pengguna baru dengan preferensi: ['budaya', 'seni'] (NCF probe)\n",
            "\n",
            "Top 5 Rekomendasi Hybrid (Kategori + NCF Probe) untuk Pengguna Baru:\n",
            "   ID Tempat                   Nama Wisata  Overall Rating (Google Maps)  google_rating_norm  avg_ncf_appeal_score  final_score %  budaya  seni\n",
            "23      T024         Museum Astana Oentara                           4.9               1.000              0.999776      99.988794       1     0\n",
            "10      T011        Tumurun Private Museum                           4.8               0.875              0.999982      93.749091       1     1\n",
            "20      T021              Museum Lokananta                           4.8               0.875              0.999632      93.731579       1     1\n",
            "90      T091  Masjid Agung Al-Aqsha Klaten                           4.8               0.875              0.990322      93.266094       0     1\n",
            "19      T020            Pura Mangkunagaran                           4.7               0.750              0.999992      87.499583       1     0\n",
            "\n",
            ">>> SKENARIO 3 (V2 Final): Rekomendasi untuk pengguna baru, preferensi: ['teknologi', 'sains'] (filter kategori sederhana)\n",
            "Membuat rekomendasi untuk pengguna baru HANYA berdasarkan kategori: ['teknologi', 'sains'] (tanpa NCF probe).\n",
            "\n",
            "Top 3 Rekomendasi Berdasarkan Kategori (Simple Filter):\n",
            "   ID Tempat                        Nama Wisata  Overall Rating (Google Maps)  Jumlah Ulasan (Google Maps)  teknologi  sains\n",
            "66      T067        Sadeyan Desa Wisata Edukasi                           5.0                            2          1      1\n",
            "46      T047  Agrowisata Strawberry Tawangmangu                           4.7                          163          0      1\n",
            "6       T007                      De Tjolomadoe                           4.6                        21515          1      0\n"
          ]
        }
      ],
      "source": [
        "import pandas as pd\n",
        "from sklearn.preprocessing import LabelEncoder, MinMaxScaler\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.feature_extraction.text import TfidfVectorizer\n",
        "from sklearn.metrics.pairwise import cosine_similarity\n",
        "import random\n",
        "import numpy as np\n",
        "import tensorflow as tf\n",
        "from tensorflow.keras.models import Model\n",
        "from tensorflow.keras.layers import Input, Embedding, Dense, Flatten, Concatenate, Dropout, BatchNormalization # Tambahkan BatchNormalization\n",
        "from tensorflow.keras.optimizers import Adam\n",
        "from tensorflow.keras import regularizers # Import regularizers\n",
        "\n",
        "# Pastikan KerasTuner sudah terinstal\n",
        "try:\n",
        "    import keras_tuner as kt\n",
        "except ImportError:\n",
        "    print(\"KerasTuner not found. Installing...\")\n",
        "    import subprocess\n",
        "    import sys\n",
        "    subprocess.check_call([sys.executable, \"-m\", \"pip\", \"install\", \"keras-tuner\", \"-q\"])\n",
        "    import keras_tuner as kt\n",
        "    print(\"KerasTuner installed successfully.\")\n",
        "\n",
        "\n",
        "# --- Set Seed for Reproducibility ---\n",
        "SEED = 42\n",
        "random.seed(SEED)\n",
        "np.random.seed(SEED)\n",
        "tf.random.set_seed(SEED)\n",
        "\n",
        "# --- Load Data ---\n",
        "# GANTI DENGAN PATH FILE ANDA JIKA BERBEDA\n",
        "# Pastikan file CSV ada di direktori yang sama atau sesuaikan path.\n",
        "try:\n",
        "    df_wisata = pd.read_csv(\"Cleaned Dataset Item (tambahin feature engineering).csv\")\n",
        "    df_user = pd.read_csv(\"Cleaned Dataset User.csv\")\n",
        "    print(\"Dataset berhasil dimuat dari file CSV.\")\n",
        "except FileNotFoundError:\n",
        "    print(\"PERINGATAN: File CSV tidak ditemukan. Menggunakan data dummy untuk demonstrasi.\")\n",
        "    print(\"Pastikan file 'Cleaned Dataset Item (tambahin feature engineering).csv' dan 'Cleaned Dataset User.csv' ada.\")\n",
        "    # Membuat DataFrame dummy jika file tidak ditemukan\n",
        "    data_wisata = {\n",
        "        'ID Tempat': [f'item_{i}' for i in range(109)],\n",
        "        'Nama Wisata': [f'Wisata Dummy {i}' for i in range(109)],\n",
        "        'Kategori': ['budaya, seni', 'lingkungan', 'sejarah', 'budaya', 'teknologi'] * 20 + ['budaya'] * 9,\n",
        "        'Kategori Umur': ['Semua Umur', 'Remaja', 'Anak-anak', 'Remaja', 'Semua Umur'] * 20 + ['Semua Umur'] * 9,\n",
        "        'Deskripsi Cleaned': ['deskripsi dummy ' + str(i) for i in range(109)],\n",
        "        'Aktivitas Cleaned': ['aktivitas dummy ' + str(i) for i in range(109)],\n",
        "        'Fasilitas Cleaned': ['fasilitas dummy ' + str(i) for i in range(109)],\n",
        "        'Overall Rating (Google Maps)': np.random.uniform(3.0, 5.0, 109).round(1),\n",
        "        'Jumlah Ulasan (Google Maps)': np.random.randint(10, 1000, 109)\n",
        "    }\n",
        "    df_wisata = pd.DataFrame(data_wisata)\n",
        "    # Buat beberapa user dan rating dummy\n",
        "    user_ids_dummy = [f'user_dummy_{i}' for i in range(50)]\n",
        "    item_ids_dummy = df_wisata['ID Tempat'].tolist()\n",
        "    ratings_data_dummy = []\n",
        "    for user_id in user_ids_dummy:\n",
        "        num_ratings = random.randint(5, 20)\n",
        "        rated_items = random.sample(item_ids_dummy, num_ratings)\n",
        "        for item_id in rated_items:\n",
        "            ratings_data_dummy.append({'ID User': user_id, 'ID Tempat': item_id, 'rating': random.randint(1, 5)})\n",
        "    df_user = pd.DataFrame(ratings_data_dummy)\n",
        "\n",
        "# --- Preprocessing df_user ---\n",
        "df_user_cleaned = df_user[['ID User', 'ID Tempat', 'rating']].dropna()\n",
        "df_user_cleaned = df_user_cleaned.drop_duplicates()\n",
        "\n",
        "user_encoder = LabelEncoder()\n",
        "item_encoder = LabelEncoder()\n",
        "\n",
        "# Fit item_encoder pada SEMUA ID Tempat dari df_wisata\n",
        "item_encoder.fit(df_wisata['ID Tempat'])\n",
        "\n",
        "# Pastikan semua ID Tempat di df_user_cleaned dikenal oleh item_encoder\n",
        "df_user_cleaned = df_user_cleaned[df_user_cleaned['ID Tempat'].isin(item_encoder.classes_)]\n",
        "\n",
        "# Fit user_encoder hanya pada user yang memiliki rating (setelah filter item)\n",
        "user_encoder.fit(df_user_cleaned['ID User'])\n",
        "\n",
        "# Pastikan semua ID User di df_user_cleaned dikenal oleh user_encoder\n",
        "df_user_cleaned = df_user_cleaned[df_user_cleaned['ID User'].isin(user_encoder.classes_)]\n",
        "\n",
        "\n",
        "# Transform ID User dan ID Tempat ke integer\n",
        "df_user_cleaned['user_id_int'] = user_encoder.transform(df_user_cleaned['ID User'])\n",
        "df_user_cleaned['item_id_int'] = item_encoder.transform(df_user_cleaned['ID Tempat'])\n",
        "\n",
        "num_users = len(user_encoder.classes_)\n",
        "num_items = len(item_encoder.classes_) # Ini adalah jumlah item unik di df_wisata\n",
        "\n",
        "# Normalisasi rating\n",
        "min_rating = df_user_cleaned['rating'].min()\n",
        "max_rating = df_user_cleaned['rating'].max()\n",
        "if max_rating == min_rating: # Hindari pembagian dengan nol jika semua rating sama\n",
        "    df_user_cleaned['rating_norm'] = 0.5 if min_rating > 0 else 0.0\n",
        "else:\n",
        "    df_user_cleaned['rating_norm'] = (df_user_cleaned['rating'] - min_rating) / (max_rating - min_rating)\n",
        "\n",
        "print(f\"\\nJumlah User Unik setelah cleaning: {num_users}\")\n",
        "print(f\"Jumlah Tempat Wisata Unik di df_wisata (digunakan item_encoder): {len(item_encoder.classes_)}\")\n",
        "print(f\"Jumlah Tempat Wisata Unik yang memiliki rating di df_user_cleaned: {df_user_cleaned['item_id_int'].nunique()}\")\n",
        "\n",
        "\n",
        "# --- Feature Engineering dan Preprocessing df_wisata ---\n",
        "# One-hot Encoding untuk kategori\n",
        "df_encoded_kategori = df_wisata['Kategori'].str.get_dummies(sep=', ')\n",
        "df_wisata = pd.concat([df_wisata, df_encoded_kategori], axis=1)\n",
        "df_wisata.drop(columns=['Kategori'], inplace=True)\n",
        "\n",
        "df_encoded_umur = df_wisata['Kategori Umur'].str.get_dummies(sep=', ')\n",
        "df_wisata = pd.concat([df_wisata, df_encoded_umur], axis=1)\n",
        "df_wisata.drop(columns=['Kategori Umur'], inplace=True)\n",
        "\n",
        "df_wisata['text_features'] = (\n",
        "    df_wisata['Deskripsi Cleaned'].fillna('') + ' ' +\n",
        "    df_wisata['Aktivitas Cleaned'].fillna('') + ' ' +\n",
        "    df_wisata['Fasilitas Cleaned'].fillna('')\n",
        ")\n",
        "\n",
        "# TF-IDF dan Cosine Similarity\n",
        "tfidf = TfidfVectorizer()\n",
        "tfidf_matrix = tfidf.fit_transform(df_wisata['text_features'])\n",
        "cosine_sim_matrix = cosine_similarity(tfidf_matrix, tfidf_matrix)\n",
        "\n",
        "# Definisikan LIST_KATEGORI_WISATA berdasarkan kolom yang ada setelah get_dummies\n",
        "ALL_GENERATED_KATEGORI_COLUMNS = list(df_encoded_kategori.columns)\n",
        "ALL_GENERATED_UMUR_COLUMNS = list(df_encoded_umur.columns)\n",
        "\n",
        "LIST_KATEGORI_WISATA_DEFINED = [ # Kategori yang secara eksplisit ingin digunakan jika ada\n",
        "    'budaya', 'kreativitas', 'lingkungan', 'religi', 'sains',\n",
        "    'sejarah', 'seni', 'teknologi'\n",
        "]\n",
        "# Filter hanya kategori yang memang ada di df_wisata setelah one-hot encoding\n",
        "EXISTING_LIST_KATEGORI_WISATA = [col for col in LIST_KATEGORI_WISATA_DEFINED if col in df_wisata.columns]\n",
        "\n",
        "feature_columns = (\n",
        "    ['Overall Rating (Google Maps)'] +\n",
        "    [col for col in EXISTING_LIST_KATEGORI_WISATA if col in df_wisata.columns] +\n",
        "    [col for col in ALL_GENERATED_UMUR_COLUMNS if col in df_wisata.columns]\n",
        ")\n",
        "feature_columns = sorted(list(set(feature_columns))) # Hapus duplikat dan urutkan\n",
        "\n",
        "df_wisata_features = df_wisata[['ID Tempat'] + feature_columns].copy()\n",
        "# Transform ID Tempat di df_wisata_features menggunakan encoder yang sama\n",
        "df_wisata_features['item_id_int'] = item_encoder.transform(df_wisata_features['ID Tempat'])\n",
        "df_wisata_features = df_wisata_features.drop(columns=['ID Tempat'])\n",
        "\n",
        "# Normalisasi 'Overall Rating'\n",
        "scaler = MinMaxScaler()\n",
        "if 'Overall Rating (Google Maps)' in df_wisata_features.columns:\n",
        "    df_wisata_features['Overall Rating (Google Maps)'] = scaler.fit_transform(\n",
        "        df_wisata_features[['Overall Rating (Google Maps)']]\n",
        "    )\n",
        "else:\n",
        "    print(\"Peringatan: Kolom 'Overall Rating (Google Maps)' tidak ditemukan di df_wisata_features.\")\n",
        "\n",
        "\n",
        "# --- Menggabungkan Data ---\n",
        "df_final = pd.merge(df_user_cleaned, df_wisata_features, on='item_id_int', how='left')\n",
        "df_final.fillna(0, inplace=True) # Isi NaN dengan 0 untuk fitur numerik\n",
        "\n",
        "# --- Membagi Data ---\n",
        "train_df, test_df = train_test_split(df_final, test_size=0.2, random_state=SEED)\n",
        "\n",
        "X_train = {\n",
        "    'user_input': train_df['user_id_int'].values,\n",
        "    'item_input': train_df['item_id_int'].values,\n",
        "    'features_input': train_df[feature_columns].values.astype(np.float32)\n",
        "}\n",
        "y_train = train_df['rating_norm'].values.astype(np.float32)\n",
        "\n",
        "X_test = {\n",
        "    'user_input': test_df['user_id_int'].values,\n",
        "    'item_input': test_df['item_id_int'].values,\n",
        "    'features_input': test_df[feature_columns].values.astype(np.float32)\n",
        "}\n",
        "y_test = test_df['rating_norm'].values.astype(np.float32)\n",
        "\n",
        "num_item_features = len(feature_columns)\n",
        "print(f\"\\nJumlah Fitur Item (setelah validasi kolom): {num_item_features}\")\n",
        "print(f\"Nama Kolom Fitur: {feature_columns}\")\n",
        "\n",
        "\n",
        "# --- Model Building Function for KerasTuner (dengan L2 Regularization & Batch Norm) ---\n",
        "def build_hybrid_ncf_model_for_tuner_v2(hp, num_users_static, num_items_static, num_item_features_static):\n",
        "    \"\"\"Membangun model Hybrid NCF untuk KerasTuner dengan L2 Regularization dan Batch Norm.\"\"\"\n",
        "\n",
        "    # Hyperparameters to tune\n",
        "    embedding_dim = hp.Int('embedding_dim', min_value=32, max_value=96, step=16)\n",
        "    mlp_units_1 = hp.Int('mlp_units_1', min_value=32, max_value=96, step=16)\n",
        "    mlp_units_2 = hp.Int('mlp_units_2', min_value=16, max_value=64, step=16)\n",
        "    mlp_units_3 = hp.Int('mlp_units_3', min_value=8, max_value=32, step=8)\n",
        "    dropout_rate = hp.Float('dropout_rate', min_value=0.2, max_value=0.5, step=0.05) # Penyesuaian step\n",
        "    learning_rate = hp.Choice('learning_rate', values=[1e-3, 5e-4, 1e-4])\n",
        "\n",
        "    feature_mlp_units_1 = hp.Int('feature_mlp_units_1', min_value=16, max_value=48, step=16)\n",
        "    feature_mlp_units_2 = hp.Int('feature_mlp_units_2', min_value=8, max_value=32, step=8)\n",
        "\n",
        "    l2_reg_factor = hp.Choice('l2_reg_factor', values=[1e-4, 1e-5, 1e-6, 0.0])\n",
        "\n",
        "    # Input Layers\n",
        "    user_input_layer = Input(shape=(1,), name='user_input')\n",
        "    item_input_layer = Input(shape=(1,), name='item_input')\n",
        "    features_input_layer = Input(shape=(num_item_features_static,), name='features_input')\n",
        "\n",
        "    # Embedding Layers\n",
        "    user_embedding_layer = Embedding(\n",
        "        input_dim=num_users_static,\n",
        "        output_dim=embedding_dim,\n",
        "        name='user_embedding',\n",
        "        embeddings_regularizer=regularizers.l2(l2_reg_factor)\n",
        "    )(user_input_layer)\n",
        "    item_embedding_layer = Embedding(\n",
        "        input_dim=num_items_static, # Seharusnya num_items_static (jumlah item unik dari encoder)\n",
        "        output_dim=embedding_dim,\n",
        "        name='item_embedding',\n",
        "        embeddings_regularizer=regularizers.l2(l2_reg_factor)\n",
        "    )(item_input_layer)\n",
        "\n",
        "    user_vec = Flatten(name='flatten_user')(user_embedding_layer)\n",
        "    item_vec = Flatten(name='flatten_item')(item_embedding_layer)\n",
        "\n",
        "    # MLP untuk Fitur Item\n",
        "    feature_mlp = Dense(\n",
        "        feature_mlp_units_1, activation='relu', kernel_regularizer=regularizers.l2(l2_reg_factor)\n",
        "    )(features_input_layer)\n",
        "    feature_mlp = Dropout(dropout_rate)(feature_mlp)\n",
        "    feature_mlp = Dense(\n",
        "        feature_mlp_units_2, activation='relu', kernel_regularizer=regularizers.l2(l2_reg_factor)\n",
        "    )(feature_mlp)\n",
        "\n",
        "    concat_layer = Concatenate()([user_vec, item_vec, feature_mlp])\n",
        "    concat_bn = BatchNormalization()(concat_layer)\n",
        "\n",
        "    # MLP Layers\n",
        "    mlp = Dense(\n",
        "        mlp_units_1, activation='relu', kernel_regularizer=regularizers.l2(l2_reg_factor)\n",
        "    )(concat_bn)\n",
        "    mlp = BatchNormalization()(mlp)\n",
        "    mlp = Dropout(dropout_rate)(mlp)\n",
        "\n",
        "    mlp = Dense(\n",
        "        mlp_units_2, activation='relu', kernel_regularizer=regularizers.l2(l2_reg_factor)\n",
        "    )(mlp)\n",
        "    mlp = BatchNormalization()(mlp)\n",
        "    mlp = Dropout(dropout_rate)(mlp)\n",
        "\n",
        "    mlp = Dense(\n",
        "        mlp_units_3, activation='relu', kernel_regularizer=regularizers.l2(l2_reg_factor)\n",
        "    )(mlp)\n",
        "\n",
        "    output_layer = Dense(1, activation='sigmoid', name='output')(mlp)\n",
        "\n",
        "    model = Model(inputs=[user_input_layer, item_input_layer, features_input_layer], outputs=output_layer)\n",
        "    model.compile(optimizer=Adam(learning_rate=learning_rate),\n",
        "                  loss='mean_squared_error',\n",
        "                  metrics=['mae'])\n",
        "    return model\n",
        "\n",
        "# --- Hyperparameter Tuning Setup ---\n",
        "if num_users == 0 or num_items == 0:\n",
        "    print(\"Peringatan: num_users atau num_items adalah nol. Embedding layer mungkin error.\")\n",
        "    best_model_tuned_v2 = None # Inisialisasi jika tidak bisa lanjut\n",
        "else:\n",
        "    build_fn_with_static_args_v2 = lambda hp: build_hybrid_ncf_model_for_tuner_v2(\n",
        "        hp,\n",
        "        num_users_static=num_users,\n",
        "        num_items_static=num_items, # Pastikan ini adalah jumlah item unik dari item_encoder\n",
        "        num_item_features_static=num_item_features\n",
        "    )\n",
        "\n",
        "    tuner_v2 = kt.RandomSearch(\n",
        "        build_fn_with_static_args_v2,\n",
        "        objective='val_mae',\n",
        "        max_trials=15,  # Kurangi sedikit untuk iterasi lebih cepat, bisa dinaikkan lagi\n",
        "        executions_per_trial=1,\n",
        "        directory='ncf_tuning_v2_final',\n",
        "        project_name='hybrid_ncf_reg_bn_final',\n",
        "        overwrite=True\n",
        "    )\n",
        "\n",
        "    tuner_v2.search_space_summary()\n",
        "\n",
        "    early_stopping_cb = tf.keras.callbacks.EarlyStopping(\n",
        "        monitor='val_mae',\n",
        "        patience=10,\n",
        "        restore_best_weights=True,\n",
        "        verbose=1\n",
        "    )\n",
        "\n",
        "    print(\"\\nMemulai Hyperparameter Tuning (Versi 2 dengan Regularisasi & Batch Norm)...\")\n",
        "    if X_train['user_input'].shape[0] > 0 and y_train.shape[0] > 0:\n",
        "        tuner_v2.search(\n",
        "            X_train,\n",
        "            y_train,\n",
        "            epochs=60, # Naikkan sedikit epoch per trial\n",
        "            validation_data=(X_test, y_test),\n",
        "            callbacks=[early_stopping_cb],\n",
        "            batch_size=128,\n",
        "            verbose=1\n",
        "        )\n",
        "\n",
        "        best_hps_v2 = tuner_v2.get_best_hyperparameters(num_trials=1)[0]\n",
        "\n",
        "        print(f\"\"\"\n",
        "        Hyperparameter terbaik (V2 Final) yang ditemukan:\n",
        "        Embedding Dim: {best_hps_v2.get('embedding_dim')}\n",
        "        MLP Unit 1: {best_hps_v2.get('mlp_units_1')}\n",
        "        MLP Unit 2: {best_hps_v2.get('mlp_units_2')}\n",
        "        MLP Unit 3: {best_hps_v2.get('mlp_units_3')}\n",
        "        Feature MLP Unit 1: {best_hps_v2.get('feature_mlp_units_1')}\n",
        "        Feature MLP Unit 2: {best_hps_v2.get('feature_mlp_units_2')}\n",
        "        Dropout Rate: {best_hps_v2.get('dropout_rate'):.3f}\n",
        "        Learning Rate: {best_hps_v2.get('learning_rate')}\n",
        "        L2 Reg Factor: {best_hps_v2.get('l2_reg_factor')}\n",
        "        \"\"\")\n",
        "\n",
        "        best_model_tuned_v2 = tuner_v2.hypermodel.build(best_hps_v2)\n",
        "\n",
        "        print(\"\\nMelatih model terbaik (V2 Final) dengan hyperparameter yang ditemukan...\")\n",
        "        final_early_stopping_cb = tf.keras.callbacks.EarlyStopping(\n",
        "            monitor='val_mae', patience=15, restore_best_weights=True, verbose=1\n",
        "        )\n",
        "\n",
        "        history_best_tuned_v2 = best_model_tuned_v2.fit(\n",
        "            X_train,\n",
        "            y_train,\n",
        "            batch_size=128,\n",
        "            epochs=120, # Latih lebih lama untuk model final\n",
        "            verbose=1,\n",
        "            validation_data=(X_test, y_test),\n",
        "            callbacks=[final_early_stopping_cb]\n",
        "        )\n",
        "\n",
        "        loss_tuned_v2, mae_tuned_v2 = best_model_tuned_v2.evaluate(X_test, y_test, verbose=0)\n",
        "        print(f\"\\nModel Tuned (V2 Final) - Test Loss: {loss_tuned_v2:.4f}\")\n",
        "        print(f\"Model Tuned (V2 Final) - Test MAE: {mae_tuned_v2:.4f}\")\n",
        "\n",
        "        # Simpan model\n",
        "        # best_model_tuned_v2.save(\"best_hybrid_ncf_model_v2_final.keras\")\n",
        "        # print(\"Model terbaik (V2 Final) disimpan sebagai 'best_hybrid_ncf_model_v2_final.keras'\")\n",
        "    else:\n",
        "        print(\"Tidak ada data training yang cukup untuk memulai tuning V2.\")\n",
        "        best_model_tuned_v2 = None\n",
        "\n",
        "\n",
        "# --- Fungsi Rekomendasi (Sama seperti sebelumnya, hanya pastikan menggunakan model yang tepat) ---\n",
        "def normalize_series_min_max(series):\n",
        "    min_val = series.min()\n",
        "    max_val = series.max()\n",
        "    if max_val == min_val:\n",
        "        return pd.Series(np.zeros_like(series, dtype=float), index=series.index) if min_val == 0 else pd.Series(np.ones_like(series, dtype=float), index=series.index)\n",
        "    return (series - min_val) / (max_val - min_val)\n",
        "\n",
        "def get_unified_recommendations(\n",
        "    raw_user_id=None,\n",
        "    preferred_categories=None,\n",
        "    n=10,\n",
        "    ncf_model=None,\n",
        "    user_encoder_passed=None,\n",
        "    item_encoder_passed=None,\n",
        "    df_user_cleaned_passed=None,\n",
        "    df_wisata_features_passed=None,\n",
        "    feature_columns_ncf_passed=None,\n",
        "    cosine_sim_matrix_passed=None,\n",
        "    df_wisata_source_passed=None, # df_wisata asli untuk info detail & filter kategori\n",
        "    k_existing=20,\n",
        "    liked_rating_threshold=4.0, # disesuaikan dengan skala rating asli\n",
        "    ncf_weight=0.7,\n",
        "    content_weight=0.3,\n",
        "    probe_user_raw_ids=None,\n",
        "    weight_google_rating_new=0.5,\n",
        "    weight_ncf_appeal_new=0.5,\n",
        "    list_kategori_wisata_valid_passed=None # Untuk memastikan kategori valid\n",
        "):\n",
        "    if ncf_model is None:\n",
        "        print(\"Error: Model NCF belum dilatih atau tidak disediakan.\")\n",
        "        return pd.DataFrame()\n",
        "    if user_encoder_passed is None or item_encoder_passed is None or df_user_cleaned_passed is None or \\\n",
        "       df_wisata_features_passed is None or feature_columns_ncf_passed is None or df_wisata_source_passed is None or \\\n",
        "       list_kategori_wisata_valid_passed is None:\n",
        "        print(\"Error: Satu atau lebih dataframes/encoders/list kategori penting tidak disediakan.\")\n",
        "        return pd.DataFrame()\n",
        "\n",
        "    is_known_user_with_reviews = False\n",
        "    user_id_int = -1\n",
        "\n",
        "    if raw_user_id:\n",
        "        try:\n",
        "            user_id_int = user_encoder_passed.transform([raw_user_id])[0]\n",
        "            if not df_user_cleaned_passed[df_user_cleaned_passed['user_id_int'] == user_id_int].empty:\n",
        "                is_known_user_with_reviews = True\n",
        "            else:\n",
        "                print(f\"Info: User ID '{raw_user_id}' dikenal encoder, tapi tidak ada riwayat review.\")\n",
        "        except ValueError:\n",
        "            print(f\"Info: User ID '{raw_user_id}' tidak dikenal encoder.\")\n",
        "\n",
        "    # --- Jalur 1: Pengguna Dikenal dan Punya Riwayat Review ---\n",
        "    if is_known_user_with_reviews:\n",
        "        print(f\"Membuat rekomendasi untuk pengguna yang sudah dikenal: {raw_user_id}\")\n",
        "        items_rated_by_user_int = df_user_cleaned_passed[df_user_cleaned_passed['user_id_int'] == user_id_int]['item_id_int'].unique()\n",
        "        all_possible_item_ids_int = df_wisata_features_passed['item_id_int'].unique()\n",
        "        items_to_predict_int = np.setdiff1d(all_possible_item_ids_int, items_rated_by_user_int, assume_unique=True)\n",
        "\n",
        "        if len(items_to_predict_int) == 0:\n",
        "            print(f\"User '{raw_user_id}' sudah memberi rating semua item atau tidak ada item lain untuk diprediksi.\")\n",
        "            return pd.DataFrame()\n",
        "\n",
        "        items_to_predict_df = pd.DataFrame({'item_id_int': items_to_predict_int})\n",
        "        batch_features_df = pd.merge(items_to_predict_df, df_wisata_features_passed, on='item_id_int', how='left')\n",
        "        batch_features_df.fillna(0, inplace=True) # Pastikan fitur diisi jika ada NaN\n",
        "        features_input_batch = batch_features_df[feature_columns_ncf_passed].values.astype(np.float32)\n",
        "\n",
        "        model_input_batch = {\n",
        "            'user_input': np.full(len(items_to_predict_int), user_id_int),\n",
        "            'item_input': items_to_predict_int,\n",
        "            'features_input': features_input_batch\n",
        "        }\n",
        "        predicted_norm_ratings_batch = ncf_model.predict(model_input_batch, verbose=0).flatten()\n",
        "\n",
        "        ncf_results_df = pd.DataFrame({\n",
        "            'item_id_int': items_to_predict_int,\n",
        "            'ncf_score_norm': predicted_norm_ratings_batch\n",
        "        })\n",
        "        ncf_top_k_candidates = ncf_results_df.sort_values(by='ncf_score_norm', ascending=False).head(k_existing)\n",
        "\n",
        "        liked_items_df = df_user_cleaned_passed[\n",
        "            (df_user_cleaned_passed['user_id_int'] == user_id_int) &\n",
        "            (df_user_cleaned_passed['rating'] >= liked_rating_threshold) # Gunakan rating asli\n",
        "        ]\n",
        "        liked_item_ids_int = liked_items_df['item_id_int'].unique()\n",
        "\n",
        "        content_affinity_scores = []\n",
        "        if len(liked_item_ids_int) > 0 and cosine_sim_matrix_passed is not None:\n",
        "            for candidate_item_id_int in ncf_top_k_candidates['item_id_int']:\n",
        "                if not (0 <= candidate_item_id_int < cosine_sim_matrix_passed.shape[0]):\n",
        "                    avg_sim = 0.0\n",
        "                else:\n",
        "                    valid_liked_indices = liked_item_ids_int[(liked_item_ids_int >= 0) & (liked_item_ids_int < cosine_sim_matrix_passed.shape[1])]\n",
        "                    if len(valid_liked_indices) > 0:\n",
        "                        sim_scores_for_candidate = cosine_sim_matrix_passed[candidate_item_id_int, valid_liked_indices]\n",
        "                        sim_scores_for_candidate = sim_scores_for_candidate[np.isfinite(sim_scores_for_candidate)]\n",
        "                        avg_sim = np.mean(sim_scores_for_candidate) if len(sim_scores_for_candidate) > 0 else 0.0\n",
        "                    else: avg_sim = 0.0\n",
        "                content_affinity_scores.append(avg_sim)\n",
        "        else:\n",
        "            content_affinity_scores = [0.0] * len(ncf_top_k_candidates)\n",
        "\n",
        "        ncf_top_k_candidates['content_affinity_score'] = content_affinity_scores\n",
        "        ncf_top_k_candidates['combined_score %'] = (\n",
        "            (ncf_weight * ncf_top_k_candidates['ncf_score_norm'] +\n",
        "            content_weight * ncf_top_k_candidates['content_affinity_score']) * 100\n",
        "        )\n",
        "        final_recommendations_df = ncf_top_k_candidates.sort_values(by='combined_score %', ascending=False)\n",
        "        top_n_final = final_recommendations_df.head(n)\n",
        "\n",
        "        # Map item_id_int kembali ke ID Tempat asli\n",
        "        item_id_int_to_raw_map = pd.Series(item_encoder_passed.classes_, index=item_encoder_passed.transform(item_encoder_passed.classes_))\n",
        "        top_n_final['ID Tempat'] = top_n_final['item_id_int'].map(item_id_int_to_raw_map)\n",
        "\n",
        "        recommendations = pd.merge(top_n_final, df_wisata_source_passed[['ID Tempat', 'Nama Wisata']], on='ID Tempat', how='left')\n",
        "        print_columns = ['ID Tempat', 'Nama Wisata', 'ncf_score_norm', 'content_affinity_score', 'combined_score %']\n",
        "        print(f\"\\nTop {n} Rekomendasi Gabungan (NCF + Cosine Boost) untuk User '{raw_user_id}':\")\n",
        "        print(recommendations[[col for col in print_columns if col in recommendations.columns]].to_string())\n",
        "        return recommendations\n",
        "\n",
        "    # --- Jalur 2: Pengguna Baru/Tidak Dikenal dengan Preferensi Kategori (Menggunakan NCF Probe) ---\n",
        "    elif preferred_categories and probe_user_raw_ids:\n",
        "        print(f\"Membuat rekomendasi untuk pengguna baru dengan preferensi: {preferred_categories} (NCF probe)\")\n",
        "        valid_preferred_categories = [cat for cat in preferred_categories if cat in df_wisata_source_passed.columns and cat in list_kategori_wisata_valid_passed]\n",
        "        if not valid_preferred_categories:\n",
        "            print(f\"Tidak ada kategori valid dari preferensi: {preferred_categories}\")\n",
        "            return pd.DataFrame()\n",
        "\n",
        "        category_match_mask = df_wisata_source_passed[valid_preferred_categories].sum(axis=1) > 0\n",
        "        candidate_wisata_df = df_wisata_source_passed[category_match_mask].copy()\n",
        "\n",
        "        if candidate_wisata_df.empty:\n",
        "            print(f\"Tidak ditemukan tempat wisata yang cocok dengan kategori: {valid_preferred_categories}\")\n",
        "            return pd.DataFrame()\n",
        "\n",
        "        if 'Overall Rating (Google Maps)' in candidate_wisata_df.columns:\n",
        "             candidate_wisata_df['google_rating_norm'] = normalize_series_min_max(candidate_wisata_df['Overall Rating (Google Maps)'])\n",
        "        else:\n",
        "             candidate_wisata_df['google_rating_norm'] = 0.0\n",
        "\n",
        "        valid_probe_user_ids_int = []\n",
        "        for raw_id_probe in probe_user_raw_ids:\n",
        "            try:\n",
        "                valid_probe_user_ids_int.append(user_encoder_passed.transform([raw_id_probe])[0])\n",
        "            except ValueError:\n",
        "                print(f\"Warning: Probe user ID '{raw_id_probe}' tidak dikenal oleh user_encoder.\")\n",
        "\n",
        "        if not valid_probe_user_ids_int:\n",
        "            print(\"Warning: Tidak ada probe user ID yang valid. NCF appeal score akan 0.\")\n",
        "            candidate_wisata_df['avg_ncf_appeal_score'] = 0.0\n",
        "        else:\n",
        "            avg_ncf_appeal_scores = []\n",
        "            for index, row_cand in candidate_wisata_df.iterrows():\n",
        "                item_id_raw_cand = row_cand['ID Tempat']\n",
        "                try:\n",
        "                    item_id_int_cand = item_encoder_passed.transform([item_id_raw_cand])[0]\n",
        "                except ValueError:\n",
        "                    avg_ncf_appeal_scores.append(0.0); continue\n",
        "\n",
        "                item_features_series = df_wisata_features_passed[df_wisata_features_passed['item_id_int'] == item_id_int_cand]\n",
        "                if item_features_series.empty:\n",
        "                    avg_ncf_appeal_scores.append(0.0); continue\n",
        "                item_features_array_cand = item_features_series[feature_columns_ncf_passed].iloc[[0]].values.astype(np.float32)\n",
        "\n",
        "                ncf_predictions_for_item = []\n",
        "                for probe_user_id_int_curr in valid_probe_user_ids_int:\n",
        "                    model_input_probe = {\n",
        "                        'user_input': np.array([probe_user_id_int_curr]),\n",
        "                        'item_input': np.array([item_id_int_cand]),\n",
        "                        'features_input': item_features_array_cand\n",
        "                    }\n",
        "                    pred = ncf_model.predict(model_input_probe, verbose=0)[0][0]\n",
        "                    ncf_predictions_for_item.append(pred)\n",
        "                avg_ncf_appeal_scores.append(np.mean(ncf_predictions_for_item) if ncf_predictions_for_item else 0.0)\n",
        "            candidate_wisata_df['avg_ncf_appeal_score'] = avg_ncf_appeal_scores\n",
        "\n",
        "        candidate_wisata_df['final_score %'] = (\n",
        "            (weight_google_rating_new * candidate_wisata_df['google_rating_norm'].fillna(0) +\n",
        "            weight_ncf_appeal_new * candidate_wisata_df['avg_ncf_appeal_score']) * 100\n",
        "        )\n",
        "        ranked_wisata = candidate_wisata_df.sort_values(by='final_score %', ascending=False).head(n)\n",
        "\n",
        "        print_columns_new = ['ID Tempat', 'Nama Wisata', 'Overall Rating (Google Maps)', 'google_rating_norm', 'avg_ncf_appeal_score', 'final_score %'] + valid_preferred_categories\n",
        "        recommendations = ranked_wisata[[col for col in print_columns_new if col in ranked_wisata.columns]]\n",
        "        print(f\"\\nTop {n} Rekomendasi Hybrid (Kategori + NCF Probe) untuk Pengguna Baru:\")\n",
        "        print(recommendations.to_string())\n",
        "        return recommendations\n",
        "\n",
        "    # --- Jalur 3: Pengguna Baru hanya dengan Preferensi Kategori (tanpa NCF probe/fallback sederhana) ---\n",
        "    elif preferred_categories:\n",
        "        print(f\"Membuat rekomendasi untuk pengguna baru HANYA berdasarkan kategori: {preferred_categories} (tanpa NCF probe).\")\n",
        "        valid_preferred_categories = [cat for cat in preferred_categories if cat in df_wisata_source_passed.columns and cat in list_kategori_wisata_valid_passed]\n",
        "        if not valid_preferred_categories:\n",
        "            print(f\"Tidak ada kategori valid dari preferensi: {preferred_categories}\")\n",
        "            return pd.DataFrame()\n",
        "\n",
        "        category_match_mask = df_wisata_source_passed[valid_preferred_categories].sum(axis=1) > 0\n",
        "        filtered_wisata = df_wisata_source_passed[category_match_mask].copy()\n",
        "\n",
        "        if filtered_wisata.empty:\n",
        "            print(f\"Tidak ditemukan tempat wisata yang cocok dengan kategori: {valid_preferred_categories}\")\n",
        "            return pd.DataFrame()\n",
        "\n",
        "        sort_cols = []\n",
        "        ascending_orders = []\n",
        "        if 'Overall Rating (Google Maps)' in filtered_wisata.columns:\n",
        "            sort_cols.append('Overall Rating (Google Maps)')\n",
        "            ascending_orders.append(False)\n",
        "        if 'Jumlah Ulasan (Google Maps)' in filtered_wisata.columns: # Jika ada kolom ini\n",
        "            sort_cols.append('Jumlah Ulasan (Google Maps)')\n",
        "            ascending_orders.append(False)\n",
        "\n",
        "        if not sort_cols: # Jika tidak ada kolom rating/ulasan, ambil saja head\n",
        "             ranked_wisata = filtered_wisata.head(n)\n",
        "        else:\n",
        "            ranked_wisata = filtered_wisata.sort_values(\n",
        "                by=sort_cols, ascending=ascending_orders\n",
        "            ).head(n)\n",
        "\n",
        "        print_cols_to_check = ['ID Tempat', 'Nama Wisata', 'Overall Rating (Google Maps)', 'Jumlah Ulasan (Google Maps)'] + valid_preferred_categories\n",
        "        recommendations = ranked_wisata[[col for col in print_cols_to_check if col in ranked_wisata.columns]]\n",
        "        print(f\"\\nTop {n} Rekomendasi Berdasarkan Kategori (Simple Filter):\")\n",
        "        print(recommendations.to_string())\n",
        "        return recommendations\n",
        "    else:\n",
        "        print(\"Tidak ada User ID atau preferensi kategori yang diberikan. Tidak dapat membuat rekomendasi.\")\n",
        "        return pd.DataFrame()\n",
        "\n",
        "\n",
        "# --- Contoh Penggunaan dengan Model yang Sudah Di-tuning (V2 Final) ---\n",
        "if 'best_model_tuned_v2' in locals() and best_model_tuned_v2 is not None:\n",
        "    if not df_user_cleaned.empty and num_users > 0:\n",
        "        # Contoh 1: Pengguna yang sudah dikenal\n",
        "        if len(user_encoder.classes_) > 0:\n",
        "            known_user_id_example = user_encoder.classes_[0]\n",
        "            print(f\"\\n>>> SKENARIO 1 (V2 Final): Rekomendasi untuk pengguna dikenal '{known_user_id_example}'\")\n",
        "            recs_known_v2 = get_unified_recommendations(\n",
        "                raw_user_id=known_user_id_example,\n",
        "                n=5,\n",
        "                ncf_model=best_model_tuned_v2,\n",
        "                user_encoder_passed=user_encoder,\n",
        "                item_encoder_passed=item_encoder,\n",
        "                df_user_cleaned_passed=df_user_cleaned,\n",
        "                df_wisata_features_passed=df_wisata_features,\n",
        "                feature_columns_ncf_passed=feature_columns,\n",
        "                cosine_sim_matrix_passed=cosine_sim_matrix,\n",
        "                df_wisata_source_passed=df_wisata,\n",
        "                liked_rating_threshold=3.5, # Sesuaikan threshold rating asli\n",
        "                list_kategori_wisata_valid_passed=EXISTING_LIST_KATEGORI_WISATA\n",
        "            )\n",
        "        else:\n",
        "            print(\"Tidak ada user yang dikenal di user_encoder untuk Skenario 1.\")\n",
        "\n",
        "        # Contoh 2: Pengguna baru dengan preferensi kategori (menggunakan NCF probe)\n",
        "        new_user_prefs_example_1 = ['budaya', 'seni'] # Gunakan kategori dari EXISTING_LIST_KATEGORI_WISATA\n",
        "        num_samples_for_probe = min(3, len(user_encoder.classes_))\n",
        "        if num_samples_for_probe > 0 :\n",
        "            probe_user_ids_for_new_example = random.sample(list(user_encoder.classes_), num_samples_for_probe)\n",
        "            print(f\"\\n>>> SKENARIO 2 (V2 Final): Rekomendasi untuk pengguna baru, preferensi: {new_user_prefs_example_1} (dengan NCF Probe)\")\n",
        "            print(f\"Probe User IDs: {probe_user_ids_for_new_example}\")\n",
        "            recs_new_ncf_probe_v2 = get_unified_recommendations(\n",
        "                preferred_categories=new_user_prefs_example_1,\n",
        "                n=5,\n",
        "                ncf_model=best_model_tuned_v2,\n",
        "                user_encoder_passed=user_encoder,\n",
        "                item_encoder_passed=item_encoder,\n",
        "                df_user_cleaned_passed=df_user_cleaned,\n",
        "                df_wisata_features_passed=df_wisata_features,\n",
        "                feature_columns_ncf_passed=feature_columns,\n",
        "                cosine_sim_matrix_passed=cosine_sim_matrix,\n",
        "                df_wisata_source_passed=df_wisata,\n",
        "                probe_user_raw_ids=probe_user_ids_for_new_example,\n",
        "                list_kategori_wisata_valid_passed=EXISTING_LIST_KATEGORI_WISATA\n",
        "            )\n",
        "        else:\n",
        "            print(\"Tidak cukup user di encoder untuk NCF Probe pada Skenario 2.\")\n",
        "    else:\n",
        "        print(\"\\nModel V2 Final telah dilatih, tetapi tidak ada data user yang valid untuk contoh rekomendasi.\")\n",
        "else:\n",
        "    print(\"\\nModel V2 Final belum dilatih atau tidak tersedia, contoh rekomendasi tidak dapat dijalankan.\")\n",
        "\n",
        "# Contoh 3: Pengguna baru hanya preferensi kategori (fallback sederhana)\n",
        "# Ini tidak memerlukan model NCF secara aktif, jadi bisa dijalankan\n",
        "new_user_prefs_example_2 = ['teknologi', 'sains'] # Gunakan kategori dari EXISTING_LIST_KATEGORI_WISATA\n",
        "print(f\"\\n>>> SKENARIO 3 (V2 Final): Rekomendasi untuk pengguna baru, preferensi: {new_user_prefs_example_2} (filter kategori sederhana)\")\n",
        "recs_new_simple_v2 = get_unified_recommendations(\n",
        "    preferred_categories=new_user_prefs_example_2,\n",
        "    n=3,\n",
        "    ncf_model=best_model_tuned_v2 if 'best_model_tuned_v2' in locals() and best_model_tuned_v2 is not None else Model(), # Berikan model dummy jika tidak ada\n",
        "    user_encoder_passed=user_encoder,\n",
        "    item_encoder_passed=item_encoder,\n",
        "    df_user_cleaned_passed=df_user_cleaned, # Diperlukan untuk konsistensi argumen\n",
        "    df_wisata_features_passed=df_wisata_features, # Diperlukan untuk konsistensi argumen\n",
        "    feature_columns_ncf_passed=feature_columns, # Diperlukan untuk konsistensi argumen\n",
        "    cosine_sim_matrix_passed=cosine_sim_matrix, # Diperlukan untuk konsistensi argumen\n",
        "    df_wisata_source_passed=df_wisata,\n",
        "    list_kategori_wisata_valid_passed=EXISTING_LIST_KATEGORI_WISATA\n",
        ")"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Save Model H5\n"
      ],
      "metadata": {
        "id": "nnC1FTaf8Q7C"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "from sklearn.preprocessing import LabelEncoder, MinMaxScaler\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.feature_extraction.text import TfidfVectorizer\n",
        "from sklearn.metrics.pairwise import cosine_similarity\n",
        "import random\n",
        "import numpy as np\n",
        "import tensorflow as tf\n",
        "from tensorflow.keras.models import Model\n",
        "from tensorflow.keras.layers import Input, Embedding, Dense, Flatten, Concatenate, Dropout, BatchNormalization # Tambahkan BatchNormalization\n",
        "from tensorflow.keras.optimizers import Adam\n",
        "from tensorflow.keras import regularizers # Import regularizers\n",
        "\n",
        "# Pastikan KerasTuner sudah terinstal\n",
        "try:\n",
        "    import keras_tuner as kt\n",
        "except ImportError:\n",
        "    print(\"KerasTuner not found. Installing...\")\n",
        "    import subprocess\n",
        "    import sys\n",
        "    subprocess.check_call([sys.executable, \"-m\", \"pip\", \"install\", \"keras-tuner\", \"-q\"])\n",
        "    import keras_tuner as kt\n",
        "    print(\"KerasTuner installed successfully.\")\n",
        "\n",
        "\n",
        "# --- Set Seed for Reproducibility ---\n",
        "SEED = 42\n",
        "random.seed(SEED)\n",
        "np.random.seed(SEED)\n",
        "tf.random.set_seed(SEED)\n",
        "\n",
        "# --- Load Data ---\n",
        "# GANTI DENGAN PATH FILE ANDA JIKA BERBEDA\n",
        "# Pastikan file CSV ada di direktori yang sama atau sesuaikan path.\n",
        "try:\n",
        "    df_wisata = pd.read_csv(\"Cleaned Dataset Item (tambahin feature engineering).csv\")\n",
        "    df_user = pd.read_csv(\"Cleaned Dataset User.csv\")\n",
        "    print(\"Dataset berhasil dimuat dari file CSV.\")\n",
        "except FileNotFoundError:\n",
        "    print(\"PERINGATAN: File CSV tidak ditemukan. Menggunakan data dummy untuk demonstrasi.\")\n",
        "    print(\"Pastikan file 'Cleaned Dataset Item (tambahin feature engineering).csv' dan 'Cleaned Dataset User.csv' ada.\")\n",
        "    # Membuat DataFrame dummy jika file tidak ditemukan\n",
        "    data_wisata = {\n",
        "        'ID Tempat': [f'item_{i}' for i in range(109)],\n",
        "        'Nama Wisata': [f'Wisata Dummy {i}' for i in range(109)],\n",
        "        'Kategori': ['budaya, seni', 'lingkungan', 'sejarah', 'budaya', 'teknologi'] * 20 + ['budaya'] * 9,\n",
        "        'Kategori Umur': ['Semua Umur', 'Remaja', 'Anak-anak', 'Remaja', 'Semua Umur'] * 20 + ['Semua Umur'] * 9,\n",
        "        'Deskripsi Cleaned': ['deskripsi dummy ' + str(i) for i in range(109)],\n",
        "        'Aktivitas Cleaned': ['aktivitas dummy ' + str(i) for i in range(109)],\n",
        "        'Fasilitas Cleaned': ['fasilitas dummy ' + str(i) for i in range(109)],\n",
        "        'Overall Rating (Google Maps)': np.random.uniform(3.0, 5.0, 109).round(1),\n",
        "        'Jumlah Ulasan (Google Maps)': np.random.randint(10, 1000, 109)\n",
        "    }\n",
        "    df_wisata = pd.DataFrame(data_wisata)\n",
        "    # Buat beberapa user dan rating dummy\n",
        "    user_ids_dummy = [f'user_dummy_{i}' for i in range(50)]\n",
        "    item_ids_dummy = df_wisata['ID Tempat'].tolist()\n",
        "    ratings_data_dummy = []\n",
        "    for user_id in user_ids_dummy:\n",
        "        num_ratings = random.randint(5, 20)\n",
        "        rated_items = random.sample(item_ids_dummy, num_ratings)\n",
        "        for item_id in rated_items:\n",
        "            ratings_data_dummy.append({'ID User': user_id, 'ID Tempat': item_id, 'rating': random.randint(1, 5)})\n",
        "    df_user = pd.DataFrame(ratings_data_dummy)\n",
        "\n",
        "# --- Preprocessing df_user ---\n",
        "df_user_cleaned = df_user[['ID User', 'ID Tempat', 'rating']].dropna()\n",
        "df_user_cleaned = df_user_cleaned.drop_duplicates()\n",
        "\n",
        "user_encoder = LabelEncoder()\n",
        "item_encoder = LabelEncoder()\n",
        "\n",
        "# Fit item_encoder pada SEMUA ID Tempat dari df_wisata\n",
        "item_encoder.fit(df_wisata['ID Tempat'])\n",
        "\n",
        "# Pastikan semua ID Tempat di df_user_cleaned dikenal oleh item_encoder\n",
        "df_user_cleaned = df_user_cleaned[df_user_cleaned['ID Tempat'].isin(item_encoder.classes_)]\n",
        "\n",
        "# Fit user_encoder hanya pada user yang memiliki rating (setelah filter item)\n",
        "user_encoder.fit(df_user_cleaned['ID User'])\n",
        "\n",
        "# Pastikan semua ID User di df_user_cleaned dikenal oleh user_encoder\n",
        "df_user_cleaned = df_user_cleaned[df_user_cleaned['ID User'].isin(user_encoder.classes_)]\n",
        "\n",
        "\n",
        "# Transform ID User dan ID Tempat ke integer\n",
        "df_user_cleaned['user_id_int'] = user_encoder.transform(df_user_cleaned['ID User'])\n",
        "df_user_cleaned['item_id_int'] = item_encoder.transform(df_user_cleaned['ID Tempat'])\n",
        "\n",
        "num_users = len(user_encoder.classes_)\n",
        "num_items = len(item_encoder.classes_) # Ini adalah jumlah item unik di df_wisata\n",
        "\n",
        "# Normalisasi rating\n",
        "min_rating = df_user_cleaned['rating'].min()\n",
        "max_rating = df_user_cleaned['rating'].max()\n",
        "if max_rating == min_rating: # Hindari pembagian dengan nol jika semua rating sama\n",
        "    df_user_cleaned['rating_norm'] = 0.5 if min_rating > 0 else 0.0\n",
        "else:\n",
        "    df_user_cleaned['rating_norm'] = (df_user_cleaned['rating'] - min_rating) / (max_rating - min_rating)\n",
        "\n",
        "print(f\"\\nJumlah User Unik setelah cleaning: {num_users}\")\n",
        "print(f\"Jumlah Tempat Wisata Unik di df_wisata (digunakan item_encoder): {len(item_encoder.classes_)}\")\n",
        "print(f\"Jumlah Tempat Wisata Unik yang memiliki rating di df_user_cleaned: {df_user_cleaned['item_id_int'].nunique()}\")\n",
        "\n",
        "\n",
        "# --- Feature Engineering dan Preprocessing df_wisata ---\n",
        "# One-hot Encoding untuk kategori\n",
        "df_encoded_kategori = df_wisata['Kategori'].str.get_dummies(sep=', ')\n",
        "df_wisata = pd.concat([df_wisata, df_encoded_kategori], axis=1)\n",
        "df_wisata.drop(columns=['Kategori'], inplace=True)\n",
        "\n",
        "df_encoded_umur = df_wisata['Kategori Umur'].str.get_dummies(sep=', ')\n",
        "df_wisata = pd.concat([df_wisata, df_encoded_umur], axis=1)\n",
        "df_wisata.drop(columns=['Kategori Umur'], inplace=True)\n",
        "\n",
        "df_wisata['text_features'] = (\n",
        "    df_wisata['Deskripsi Cleaned'].fillna('') + ' ' +\n",
        "    df_wisata['Aktivitas Cleaned'].fillna('') + ' ' +\n",
        "    df_wisata['Fasilitas Cleaned'].fillna('')\n",
        ")\n",
        "\n",
        "# TF-IDF dan Cosine Similarity\n",
        "tfidf = TfidfVectorizer()\n",
        "tfidf_matrix = tfidf.fit_transform(df_wisata['text_features'])\n",
        "cosine_sim_matrix = cosine_similarity(tfidf_matrix, tfidf_matrix)\n",
        "\n",
        "# Definisikan LIST_KATEGORI_WISATA berdasarkan kolom yang ada setelah get_dummies\n",
        "ALL_GENERATED_KATEGORI_COLUMNS = list(df_encoded_kategori.columns)\n",
        "ALL_GENERATED_UMUR_COLUMNS = list(df_encoded_umur.columns)\n",
        "\n",
        "LIST_KATEGORI_WISATA_DEFINED = [ # Kategori yang secara eksplisit ingin digunakan jika ada\n",
        "    'budaya', 'kreativitas', 'lingkungan', 'religi', 'sains',\n",
        "    'sejarah', 'seni', 'teknologi'\n",
        "]\n",
        "# Filter hanya kategori yang memang ada di df_wisata setelah one-hot encoding\n",
        "EXISTING_LIST_KATEGORI_WISATA = [col for col in LIST_KATEGORI_WISATA_DEFINED if col in df_wisata.columns]\n",
        "\n",
        "feature_columns = (\n",
        "    ['Overall Rating (Google Maps)'] +\n",
        "    [col for col in EXISTING_LIST_KATEGORI_WISATA if col in df_wisata.columns] +\n",
        "    [col for col in ALL_GENERATED_UMUR_COLUMNS if col in df_wisata.columns]\n",
        ")\n",
        "feature_columns = sorted(list(set(feature_columns))) # Hapus duplikat dan urutkan\n",
        "\n",
        "df_wisata_features = df_wisata[['ID Tempat'] + feature_columns].copy()\n",
        "# Transform ID Tempat di df_wisata_features menggunakan encoder yang sama\n",
        "df_wisata_features['item_id_int'] = item_encoder.transform(df_wisata_features['ID Tempat'])\n",
        "df_wisata_features = df_wisata_features.drop(columns=['ID Tempat'])\n",
        "\n",
        "# Normalisasi 'Overall Rating'\n",
        "scaler = MinMaxScaler()\n",
        "if 'Overall Rating (Google Maps)' in df_wisata_features.columns:\n",
        "    df_wisata_features['Overall Rating (Google Maps)'] = scaler.fit_transform(\n",
        "        df_wisata_features[['Overall Rating (Google Maps)']]\n",
        "    )\n",
        "else:\n",
        "    print(\"Peringatan: Kolom 'Overall Rating (Google Maps)' tidak ditemukan di df_wisata_features.\")\n",
        "\n",
        "\n",
        "# --- Menggabungkan Data ---\n",
        "df_final = pd.merge(df_user_cleaned, df_wisata_features, on='item_id_int', how='left')\n",
        "df_final.fillna(0, inplace=True) # Isi NaN dengan 0 untuk fitur numerik\n",
        "\n",
        "# --- Membagi Data ---\n",
        "train_df, test_df = train_test_split(df_final, test_size=0.2, random_state=SEED)\n",
        "\n",
        "X_train = {\n",
        "    'user_input': train_df['user_id_int'].values,\n",
        "    'item_input': train_df['item_id_int'].values,\n",
        "    'features_input': train_df[feature_columns].values.astype(np.float32)\n",
        "}\n",
        "y_train = train_df['rating_norm'].values.astype(np.float32)\n",
        "\n",
        "X_test = {\n",
        "    'user_input': test_df['user_id_int'].values,\n",
        "    'item_input': test_df['item_id_int'].values,\n",
        "    'features_input': test_df[feature_columns].values.astype(np.float32)\n",
        "}\n",
        "y_test = test_df['rating_norm'].values.astype(np.float32)\n",
        "\n",
        "num_item_features = len(feature_columns)\n",
        "print(f\"\\nJumlah Fitur Item (setelah validasi kolom): {num_item_features}\")\n",
        "print(f\"Nama Kolom Fitur: {feature_columns}\")\n",
        "\n",
        "\n",
        "# --- Model Building Function for KerasTuner (dengan L2 Regularization & Batch Norm) ---\n",
        "def build_hybrid_ncf_model_for_tuner_v2(hp, num_users_static, num_items_static, num_item_features_static):\n",
        "    \"\"\"Membangun model Hybrid NCF untuk KerasTuner dengan L2 Regularization dan Batch Norm.\"\"\"\n",
        "\n",
        "    # Hyperparameters to tune\n",
        "    embedding_dim = hp.Int('embedding_dim', min_value=32, max_value=96, step=16)\n",
        "    mlp_units_1 = hp.Int('mlp_units_1', min_value=32, max_value=96, step=16)\n",
        "    mlp_units_2 = hp.Int('mlp_units_2', min_value=16, max_value=64, step=16)\n",
        "    mlp_units_3 = hp.Int('mlp_units_3', min_value=8, max_value=32, step=8)\n",
        "    dropout_rate = hp.Float('dropout_rate', min_value=0.2, max_value=0.5, step=0.05) # Penyesuaian step\n",
        "    learning_rate = hp.Choice('learning_rate', values=[1e-3, 5e-4, 1e-4])\n",
        "\n",
        "    feature_mlp_units_1 = hp.Int('feature_mlp_units_1', min_value=16, max_value=48, step=16)\n",
        "    feature_mlp_units_2 = hp.Int('feature_mlp_units_2', min_value=8, max_value=32, step=8)\n",
        "\n",
        "    l2_reg_factor = hp.Choice('l2_reg_factor', values=[1e-4, 1e-5, 1e-6, 0.0])\n",
        "\n",
        "    # Input Layers\n",
        "    user_input_layer = Input(shape=(1,), name='user_input')\n",
        "    item_input_layer = Input(shape=(1,), name='item_input')\n",
        "    features_input_layer = Input(shape=(num_item_features_static,), name='features_input')\n",
        "\n",
        "    # Embedding Layers\n",
        "    user_embedding_layer = Embedding(\n",
        "        input_dim=num_users_static,\n",
        "        output_dim=embedding_dim,\n",
        "        name='user_embedding',\n",
        "        embeddings_regularizer=regularizers.l2(l2_reg_factor)\n",
        "    )(user_input_layer)\n",
        "    item_embedding_layer = Embedding(\n",
        "        input_dim=num_items_static, # Seharusnya num_items_static (jumlah item unik dari encoder)\n",
        "        output_dim=embedding_dim,\n",
        "        name='item_embedding',\n",
        "        embeddings_regularizer=regularizers.l2(l2_reg_factor)\n",
        "    )(item_input_layer)\n",
        "\n",
        "    user_vec = Flatten(name='flatten_user')(user_embedding_layer)\n",
        "    item_vec = Flatten(name='flatten_item')(item_embedding_layer)\n",
        "\n",
        "    # MLP untuk Fitur Item\n",
        "    feature_mlp = Dense(\n",
        "        feature_mlp_units_1, activation='relu', kernel_regularizer=regularizers.l2(l2_reg_factor)\n",
        "    )(features_input_layer)\n",
        "    feature_mlp = Dropout(dropout_rate)(feature_mlp)\n",
        "    feature_mlp = Dense(\n",
        "        feature_mlp_units_2, activation='relu', kernel_regularizer=regularizers.l2(l2_reg_factor)\n",
        "    )(feature_mlp)\n",
        "\n",
        "    concat_layer = Concatenate()([user_vec, item_vec, feature_mlp])\n",
        "    concat_bn = BatchNormalization()(concat_layer)\n",
        "\n",
        "    # MLP Layers\n",
        "    mlp = Dense(\n",
        "        mlp_units_1, activation='relu', kernel_regularizer=regularizers.l2(l2_reg_factor)\n",
        "    )(concat_bn)\n",
        "    mlp = BatchNormalization()(mlp)\n",
        "    mlp = Dropout(dropout_rate)(mlp)\n",
        "\n",
        "    mlp = Dense(\n",
        "        mlp_units_2, activation='relu', kernel_regularizer=regularizers.l2(l2_reg_factor)\n",
        "    )(mlp)\n",
        "    mlp = BatchNormalization()(mlp)\n",
        "    mlp = Dropout(dropout_rate)(mlp)\n",
        "\n",
        "    mlp = Dense(\n",
        "        mlp_units_3, activation='relu', kernel_regularizer=regularizers.l2(l2_reg_factor)\n",
        "    )(mlp)\n",
        "\n",
        "    output_layer = Dense(1, activation='sigmoid', name='output')(mlp)\n",
        "\n",
        "    model = Model(inputs=[user_input_layer, item_input_layer, features_input_layer], outputs=output_layer)\n",
        "    model.compile(optimizer=Adam(learning_rate=learning_rate),\n",
        "                  loss='mean_squared_error',\n",
        "                  metrics=['mae'])\n",
        "    return model\n",
        "\n",
        "# --- Hyperparameter Tuning Setup ---\n",
        "if num_users == 0 or num_items == 0:\n",
        "    print(\"Peringatan: num_users atau num_items adalah nol. Embedding layer mungkin error.\")\n",
        "    best_model_tuned_v2 = None # Inisialisasi jika tidak bisa lanjut\n",
        "else:\n",
        "    build_fn_with_static_args_v2 = lambda hp: build_hybrid_ncf_model_for_tuner_v2(\n",
        "        hp,\n",
        "        num_users_static=num_users,\n",
        "        num_items_static=num_items, # Pastikan ini adalah jumlah item unik dari item_encoder\n",
        "        num_item_features_static=num_item_features\n",
        "    )\n",
        "\n",
        "    tuner_v2 = kt.RandomSearch(\n",
        "        build_fn_with_static_args_v2,\n",
        "        objective='val_mae',\n",
        "        max_trials=15,  # Kurangi sedikit untuk iterasi lebih cepat, bisa dinaikkan lagi\n",
        "        executions_per_trial=1,\n",
        "        directory='ncf_tuning_v2_final',\n",
        "        project_name='hybrid_ncf_reg_bn_final',\n",
        "        overwrite=True\n",
        "    )\n",
        "\n",
        "    tuner_v2.search_space_summary()\n",
        "\n",
        "    early_stopping_cb = tf.keras.callbacks.EarlyStopping(\n",
        "        monitor='val_mae',\n",
        "        patience=10,\n",
        "        restore_best_weights=True,\n",
        "        verbose=1\n",
        "    )\n",
        "\n",
        "    print(\"\\nMemulai Hyperparameter Tuning (Versi 2 dengan Regularisasi & Batch Norm)...\")\n",
        "    if X_train['user_input'].shape[0] > 0 and y_train.shape[0] > 0:\n",
        "        tuner_v2.search(\n",
        "            X_train,\n",
        "            y_train,\n",
        "            epochs=60, # Naikkan sedikit epoch per trial\n",
        "            validation_data=(X_test, y_test),\n",
        "            callbacks=[early_stopping_cb],\n",
        "            batch_size=128,\n",
        "            verbose=1\n",
        "        )\n",
        "\n",
        "        best_hps_v2 = tuner_v2.get_best_hyperparameters(num_trials=1)[0]\n",
        "\n",
        "        print(f\"\"\"\n",
        "        Hyperparameter terbaik (V2 Final) yang ditemukan:\n",
        "        Embedding Dim: {best_hps_v2.get('embedding_dim')}\n",
        "        MLP Unit 1: {best_hps_v2.get('mlp_units_1')}\n",
        "        MLP Unit 2: {best_hps_v2.get('mlp_units_2')}\n",
        "        MLP Unit 3: {best_hps_v2.get('mlp_units_3')}\n",
        "        Feature MLP Unit 1: {best_hps_v2.get('feature_mlp_units_1')}\n",
        "        Feature MLP Unit 2: {best_hps_v2.get('feature_mlp_units_2')}\n",
        "        Dropout Rate: {best_hps_v2.get('dropout_rate'):.3f}\n",
        "        Learning Rate: {best_hps_v2.get('learning_rate')}\n",
        "        L2 Reg Factor: {best_hps_v2.get('l2_reg_factor')}\n",
        "        \"\"\")\n",
        "\n",
        "        best_model_tuned_v2 = tuner_v2.hypermodel.build(best_hps_v2)\n",
        "\n",
        "        print(\"\\nMelatih model terbaik (V2 Final) dengan hyperparameter yang ditemukan...\")\n",
        "        final_early_stopping_cb = tf.keras.callbacks.EarlyStopping(\n",
        "            monitor='val_mae', patience=15, restore_best_weights=True, verbose=1\n",
        "        )\n",
        "\n",
        "        history_best_tuned_v2 = best_model_tuned_v2.fit(\n",
        "            X_train,\n",
        "            y_train,\n",
        "            batch_size=128,\n",
        "            epochs=120, # Latih lebih lama untuk model final\n",
        "            verbose=1,\n",
        "            validation_data=(X_test, y_test),\n",
        "            callbacks=[final_early_stopping_cb]\n",
        "        )\n",
        "\n",
        "        loss_tuned_v2, mae_tuned_v2 = best_model_tuned_v2.evaluate(X_test, y_test, verbose=0)\n",
        "        print(f\"\\nModel Tuned (V2 Final) - Test Loss: {loss_tuned_v2:.4f}\")\n",
        "        print(f\"Model Tuned (V2 Final) - Test MAE: {mae_tuned_v2:.4f}\")\n",
        "\n",
        "        # Simpan model\n",
        "        # best_model_tuned_v2.save(\"best_hybrid_ncf_model_v2_final.keras\")\n",
        "        # print(\"Model terbaik (V2 Final) disimpan sebagai 'best_hybrid_ncf_model_v2_final.keras'\")\n",
        "    else:\n",
        "        print(\"Tidak ada data training yang cukup untuk memulai tuning V2.\")\n",
        "        best_model_tuned_v2 = None\n",
        "\n",
        "\n",
        "# --- Fungsi Rekomendasi (Sama seperti sebelumnya, hanya pastikan menggunakan model yang tepat) ---\n",
        "def normalize_series_min_max(series):\n",
        "    min_val = series.min()\n",
        "    max_val = series.max()\n",
        "    if max_val == min_val:\n",
        "        return pd.Series(np.zeros_like(series, dtype=float), index=series.index) if min_val == 0 else pd.Series(np.ones_like(series, dtype=float), index=series.index)\n",
        "    return (series - min_val) / (max_val - min_val)\n",
        "\n",
        "def get_unified_recommendations(\n",
        "    raw_user_id=None,\n",
        "    preferred_categories=None,\n",
        "    n=10,\n",
        "    ncf_model=None,\n",
        "    user_encoder_passed=None,\n",
        "    item_encoder_passed=None,\n",
        "    df_user_cleaned_passed=None,\n",
        "    df_wisata_features_passed=None,\n",
        "    feature_columns_ncf_passed=None,\n",
        "    cosine_sim_matrix_passed=None,\n",
        "    df_wisata_source_passed=None, # df_wisata asli untuk info detail & filter kategori\n",
        "    k_existing=20,\n",
        "    liked_rating_threshold=4.0, # disesuaikan dengan skala rating asli\n",
        "    ncf_weight=0.7,\n",
        "    content_weight=0.3,\n",
        "    probe_user_raw_ids=None,\n",
        "    weight_google_rating_new=0.5,\n",
        "    weight_ncf_appeal_new=0.5,\n",
        "    list_kategori_wisata_valid_passed=None # Untuk memastikan kategori valid\n",
        "):\n",
        "    if ncf_model is None:\n",
        "        print(\"Error: Model NCF belum dilatih atau tidak disediakan.\")\n",
        "        return pd.DataFrame()\n",
        "    if user_encoder_passed is None or item_encoder_passed is None or df_user_cleaned_passed is None or \\\n",
        "       df_wisata_features_passed is None or feature_columns_ncf_passed is None or df_wisata_source_passed is None or \\\n",
        "       list_kategori_wisata_valid_passed is None:\n",
        "        print(\"Error: Satu atau lebih dataframes/encoders/list kategori penting tidak disediakan.\")\n",
        "        return pd.DataFrame()\n",
        "\n",
        "    is_known_user_with_reviews = False\n",
        "    user_id_int = -1\n",
        "\n",
        "    if raw_user_id:\n",
        "        try:\n",
        "            user_id_int = user_encoder_passed.transform([raw_user_id])[0]\n",
        "            if not df_user_cleaned_passed[df_user_cleaned_passed['user_id_int'] == user_id_int].empty:\n",
        "                is_known_user_with_reviews = True\n",
        "            else:\n",
        "                print(f\"Info: User ID '{raw_user_id}' dikenal encoder, tapi tidak ada riwayat review.\")\n",
        "        except ValueError:\n",
        "            print(f\"Info: User ID '{raw_user_id}' tidak dikenal encoder.\")\n",
        "\n",
        "    # --- Jalur 1: Pengguna Dikenal dan Punya Riwayat Review ---\n",
        "    if is_known_user_with_reviews:\n",
        "        print(f\"Membuat rekomendasi untuk pengguna yang sudah dikenal: {raw_user_id}\")\n",
        "        items_rated_by_user_int = df_user_cleaned_passed[df_user_cleaned_passed['user_id_int'] == user_id_int]['item_id_int'].unique()\n",
        "        all_possible_item_ids_int = df_wisata_features_passed['item_id_int'].unique()\n",
        "        items_to_predict_int = np.setdiff1d(all_possible_item_ids_int, items_rated_by_user_int, assume_unique=True)\n",
        "\n",
        "        if len(items_to_predict_int) == 0:\n",
        "            print(f\"User '{raw_user_id}' sudah memberi rating semua item atau tidak ada item lain untuk diprediksi.\")\n",
        "            return pd.DataFrame()\n",
        "\n",
        "        items_to_predict_df = pd.DataFrame({'item_id_int': items_to_predict_int})\n",
        "        batch_features_df = pd.merge(items_to_predict_df, df_wisata_features_passed, on='item_id_int', how='left')\n",
        "        batch_features_df.fillna(0, inplace=True) # Pastikan fitur diisi jika ada NaN\n",
        "        features_input_batch = batch_features_df[feature_columns_ncf_passed].values.astype(np.float32)\n",
        "\n",
        "        model_input_batch = {\n",
        "            'user_input': np.full(len(items_to_predict_int), user_id_int),\n",
        "            'item_input': items_to_predict_int,\n",
        "            'features_input': features_input_batch\n",
        "        }\n",
        "        predicted_norm_ratings_batch = ncf_model.predict(model_input_batch, verbose=0).flatten()\n",
        "\n",
        "        ncf_results_df = pd.DataFrame({\n",
        "            'item_id_int': items_to_predict_int,\n",
        "            'ncf_score_norm': predicted_norm_ratings_batch\n",
        "        })\n",
        "        ncf_top_k_candidates = ncf_results_df.sort_values(by='ncf_score_norm', ascending=False).head(k_existing)\n",
        "\n",
        "        liked_items_df = df_user_cleaned_passed[\n",
        "            (df_user_cleaned_passed['user_id_int'] == user_id_int) &\n",
        "            (df_user_cleaned_passed['rating'] >= liked_rating_threshold) # Gunakan rating asli\n",
        "        ]\n",
        "        liked_item_ids_int = liked_items_df['item_id_int'].unique()\n",
        "\n",
        "        content_affinity_scores = []\n",
        "        if len(liked_item_ids_int) > 0 and cosine_sim_matrix_passed is not None:\n",
        "            for candidate_item_id_int in ncf_top_k_candidates['item_id_int']:\n",
        "                if not (0 <= candidate_item_id_int < cosine_sim_matrix_passed.shape[0]):\n",
        "                    avg_sim = 0.0\n",
        "                else:\n",
        "                    valid_liked_indices = liked_item_ids_int[(liked_item_ids_int >= 0) & (liked_item_ids_int < cosine_sim_matrix_passed.shape[1])]\n",
        "                    if len(valid_liked_indices) > 0:\n",
        "                        sim_scores_for_candidate = cosine_sim_matrix_passed[candidate_item_id_int, valid_liked_indices]\n",
        "                        sim_scores_for_candidate = sim_scores_for_candidate[np.isfinite(sim_scores_for_candidate)]\n",
        "                        avg_sim = np.mean(sim_scores_for_candidate) if len(sim_scores_for_candidate) > 0 else 0.0\n",
        "                    else: avg_sim = 0.0\n",
        "                content_affinity_scores.append(avg_sim)\n",
        "        else:\n",
        "            content_affinity_scores = [0.0] * len(ncf_top_k_candidates)\n",
        "\n",
        "        ncf_top_k_candidates['content_affinity_score'] = content_affinity_scores\n",
        "        ncf_top_k_candidates['combined_score %'] = (\n",
        "            (ncf_weight * ncf_top_k_candidates['ncf_score_norm'] +\n",
        "            content_weight * ncf_top_k_candidates['content_affinity_score']) * 100\n",
        "        )\n",
        "        final_recommendations_df = ncf_top_k_candidates.sort_values(by='combined_score %', ascending=False)\n",
        "        top_n_final = final_recommendations_df.head(n)\n",
        "\n",
        "        # Map item_id_int kembali ke ID Tempat asli\n",
        "        item_id_int_to_raw_map = pd.Series(item_encoder_passed.classes_, index=item_encoder_passed.transform(item_encoder_passed.classes_))\n",
        "        top_n_final['ID Tempat'] = top_n_final['item_id_int'].map(item_id_int_to_raw_map)\n",
        "\n",
        "        recommendations = pd.merge(top_n_final, df_wisata_source_passed[['ID Tempat', 'Nama Wisata']], on='ID Tempat', how='left')\n",
        "        print_columns = ['ID Tempat', 'Nama Wisata', 'ncf_score_norm', 'content_affinity_score', 'combined_score %']\n",
        "        print(f\"\\nTop {n} Rekomendasi Gabungan (NCF + Cosine Boost) untuk User '{raw_user_id}':\")\n",
        "        print(recommendations[[col for col in print_columns if col in recommendations.columns]].to_string())\n",
        "        return recommendations\n",
        "\n",
        "    # --- Jalur 2: Pengguna Baru/Tidak Dikenal dengan Preferensi Kategori (Menggunakan NCF Probe) ---\n",
        "    elif preferred_categories and probe_user_raw_ids:\n",
        "        print(f\"Membuat rekomendasi untuk pengguna baru dengan preferensi: {preferred_categories} (NCF probe)\")\n",
        "        valid_preferred_categories = [cat for cat in preferred_categories if cat in df_wisata_source_passed.columns and cat in list_kategori_wisata_valid_passed]\n",
        "        if not valid_preferred_categories:\n",
        "            print(f\"Tidak ada kategori valid dari preferensi: {preferred_categories}\")\n",
        "            return pd.DataFrame()\n",
        "\n",
        "        category_match_mask = df_wisata_source_passed[valid_preferred_categories].sum(axis=1) > 0\n",
        "        candidate_wisata_df = df_wisata_source_passed[category_match_mask].copy()\n",
        "\n",
        "        if candidate_wisata_df.empty:\n",
        "            print(f\"Tidak ditemukan tempat wisata yang cocok dengan kategori: {valid_preferred_categories}\")\n",
        "            return pd.DataFrame()\n",
        "\n",
        "        if 'Overall Rating (Google Maps)' in candidate_wisata_df.columns:\n",
        "             candidate_wisata_df['google_rating_norm'] = normalize_series_min_max(candidate_wisata_df['Overall Rating (Google Maps)'])\n",
        "        else:\n",
        "             candidate_wisata_df['google_rating_norm'] = 0.0\n",
        "\n",
        "        valid_probe_user_ids_int = []\n",
        "        for raw_id_probe in probe_user_raw_ids:\n",
        "            try:\n",
        "                valid_probe_user_ids_int.append(user_encoder_passed.transform([raw_id_probe])[0])\n",
        "            except ValueError:\n",
        "                print(f\"Warning: Probe user ID '{raw_id_probe}' tidak dikenal oleh user_encoder.\")\n",
        "\n",
        "        if not valid_probe_user_ids_int:\n",
        "            print(\"Warning: Tidak ada probe user ID yang valid. NCF appeal score akan 0.\")\n",
        "            candidate_wisata_df['avg_ncf_appeal_score'] = 0.0\n",
        "        else:\n",
        "            avg_ncf_appeal_scores = []\n",
        "            for index, row_cand in candidate_wisata_df.iterrows():\n",
        "                item_id_raw_cand = row_cand['ID Tempat']\n",
        "                try:\n",
        "                    item_id_int_cand = item_encoder_passed.transform([item_id_raw_cand])[0]\n",
        "                except ValueError:\n",
        "                    avg_ncf_appeal_scores.append(0.0); continue\n",
        "\n",
        "                item_features_series = df_wisata_features_passed[df_wisata_features_passed['item_id_int'] == item_id_int_cand]\n",
        "                if item_features_series.empty:\n",
        "                    avg_ncf_appeal_scores.append(0.0); continue\n",
        "                item_features_array_cand = item_features_series[feature_columns_ncf_passed].iloc[[0]].values.astype(np.float32)\n",
        "\n",
        "                ncf_predictions_for_item = []\n",
        "                for probe_user_id_int_curr in valid_probe_user_ids_int:\n",
        "                    model_input_probe = {\n",
        "                        'user_input': np.array([probe_user_id_int_curr]),\n",
        "                        'item_input': np.array([item_id_int_cand]),\n",
        "                        'features_input': item_features_array_cand\n",
        "                    }\n",
        "                    pred = ncf_model.predict(model_input_probe, verbose=0)[0][0]\n",
        "                    ncf_predictions_for_item.append(pred)\n",
        "                avg_ncf_appeal_scores.append(np.mean(ncf_predictions_for_item) if ncf_predictions_for_item else 0.0)\n",
        "            candidate_wisata_df['avg_ncf_appeal_score'] = avg_ncf_appeal_scores\n",
        "\n",
        "        candidate_wisata_df['final_score %'] = (\n",
        "            (weight_google_rating_new * candidate_wisata_df['google_rating_norm'].fillna(0) +\n",
        "            weight_ncf_appeal_new * candidate_wisata_df['avg_ncf_appeal_score']) * 100\n",
        "        )\n",
        "        ranked_wisata = candidate_wisata_df.sort_values(by='final_score %', ascending=False).head(n)\n",
        "\n",
        "        print_columns_new = ['ID Tempat', 'Nama Wisata', 'Overall Rating (Google Maps)', 'google_rating_norm', 'avg_ncf_appeal_score', 'final_score %'] + valid_preferred_categories\n",
        "        recommendations = ranked_wisata[[col for col in print_columns_new if col in ranked_wisata.columns]]\n",
        "        print(f\"\\nTop {n} Rekomendasi Hybrid (Kategori + NCF Probe) untuk Pengguna Baru:\")\n",
        "        print(recommendations.to_string())\n",
        "        return recommendations\n",
        "\n",
        "    # --- Jalur 3: Pengguna Baru hanya dengan Preferensi Kategori (tanpa NCF probe/fallback sederhana) ---\n",
        "    elif preferred_categories:\n",
        "        print(f\"Membuat rekomendasi untuk pengguna baru HANYA berdasarkan kategori: {preferred_categories} (tanpa NCF probe).\")\n",
        "        valid_preferred_categories = [cat for cat in preferred_categories if cat in df_wisata_source_passed.columns and cat in list_kategori_wisata_valid_passed]\n",
        "        if not valid_preferred_categories:\n",
        "            print(f\"Tidak ada kategori valid dari preferensi: {preferred_categories}\")\n",
        "            return pd.DataFrame()\n",
        "\n",
        "        category_match_mask = df_wisata_source_passed[valid_preferred_categories].sum(axis=1) > 0\n",
        "        filtered_wisata = df_wisata_source_passed[category_match_mask].copy()\n",
        "\n",
        "        if filtered_wisata.empty:\n",
        "            print(f\"Tidak ditemukan tempat wisata yang cocok dengan kategori: {valid_preferred_categories}\")\n",
        "            return pd.DataFrame()\n",
        "\n",
        "        sort_cols = []\n",
        "        ascending_orders = []\n",
        "        if 'Overall Rating (Google Maps)' in filtered_wisata.columns:\n",
        "            sort_cols.append('Overall Rating (Google Maps)')\n",
        "            ascending_orders.append(False)\n",
        "        if 'Jumlah Ulasan (Google Maps)' in filtered_wisata.columns: # Jika ada kolom ini\n",
        "            sort_cols.append('Jumlah Ulasan (Google Maps)')\n",
        "            ascending_orders.append(False)\n",
        "\n",
        "        if not sort_cols: # Jika tidak ada kolom rating/ulasan, ambil saja head\n",
        "             ranked_wisata = filtered_wisata.head(n)\n",
        "        else:\n",
        "            ranked_wisata = filtered_wisata.sort_values(\n",
        "                by=sort_cols, ascending=ascending_orders\n",
        "            ).head(n)\n",
        "\n",
        "        print_cols_to_check = ['ID Tempat', 'Nama Wisata', 'Overall Rating (Google Maps)', 'Jumlah Ulasan (Google Maps)'] + valid_preferred_categories\n",
        "        recommendations = ranked_wisata[[col for col in print_cols_to_check if col in ranked_wisata.columns]]\n",
        "        print(f\"\\nTop {n} Rekomendasi Berdasarkan Kategori (Simple Filter):\")\n",
        "        print(recommendations.to_string())\n",
        "        return recommendations\n",
        "    else:\n",
        "        print(\"Tidak ada User ID atau preferensi kategori yang diberikan. Tidak dapat membuat rekomendasi.\")\n",
        "        return pd.DataFrame()\n",
        "\n",
        "\n",
        "# --- Contoh Penggunaan dengan Model yang Sudah Di-tuning (V2 Final) ---\n",
        "if 'best_model_tuned_v2' in locals() and best_model_tuned_v2 is not None:\n",
        "    if not df_user_cleaned.empty and num_users > 0:\n",
        "        # Contoh 1: Pengguna yang sudah dikenal\n",
        "        if len(user_encoder.classes_) > 0:\n",
        "            known_user_id_example = user_encoder.classes_[0]\n",
        "            print(f\"\\n>>> SKENARIO 1 (V2 Final): Rekomendasi untuk pengguna dikenal '{known_user_id_example}'\")\n",
        "            recs_known_v2 = get_unified_recommendations(\n",
        "                raw_user_id=known_user_id_example,\n",
        "                n=5,\n",
        "                ncf_model=best_model_tuned_v2,\n",
        "                user_encoder_passed=user_encoder,\n",
        "                item_encoder_passed=item_encoder,\n",
        "                df_user_cleaned_passed=df_user_cleaned,\n",
        "                df_wisata_features_passed=df_wisata_features,\n",
        "                feature_columns_ncf_passed=feature_columns,\n",
        "                cosine_sim_matrix_passed=cosine_sim_matrix,\n",
        "                df_wisata_source_passed=df_wisata,\n",
        "                liked_rating_threshold=3.5, # Sesuaikan threshold rating asli\n",
        "                list_kategori_wisata_valid_passed=EXISTING_LIST_KATEGORI_WISATA\n",
        "            )\n",
        "        else:\n",
        "            print(\"Tidak ada user yang dikenal di user_encoder untuk Skenario 1.\")\n",
        "\n",
        "        # Contoh 2: Pengguna baru dengan preferensi kategori (menggunakan NCF probe)\n",
        "        new_user_prefs_example_1 = ['budaya', 'seni'] # Gunakan kategori dari EXISTING_LIST_KATEGORI_WISATA\n",
        "        num_samples_for_probe = min(3, len(user_encoder.classes_))\n",
        "        if num_samples_for_probe > 0 :\n",
        "            probe_user_ids_for_new_example = random.sample(list(user_encoder.classes_), num_samples_for_probe)\n",
        "            print(f\"\\n>>> SKENARIO 2 (V2 Final): Rekomendasi untuk pengguna baru, preferensi: {new_user_prefs_example_1} (dengan NCF Probe)\")\n",
        "            print(f\"Probe User IDs: {probe_user_ids_for_new_example}\")\n",
        "            recs_new_ncf_probe_v2 = get_unified_recommendations(\n",
        "                preferred_categories=new_user_prefs_example_1,\n",
        "                n=5,\n",
        "                ncf_model=best_model_tuned_v2,\n",
        "                user_encoder_passed=user_encoder,\n",
        "                item_encoder_passed=item_encoder,\n",
        "                df_user_cleaned_passed=df_user_cleaned,\n",
        "                df_wisata_features_passed=df_wisata_features,\n",
        "                feature_columns_ncf_passed=feature_columns,\n",
        "                cosine_sim_matrix_passed=cosine_sim_matrix,\n",
        "                df_wisata_source_passed=df_wisata,\n",
        "                probe_user_raw_ids=probe_user_ids_for_new_example,\n",
        "                list_kategori_wisata_valid_passed=EXISTING_LIST_KATEGORI_WISATA\n",
        "            )\n",
        "        else:\n",
        "            print(\"Tidak cukup user di encoder untuk NCF Probe pada Skenario 2.\")\n",
        "    else:\n",
        "        print(\"\\nModel V2 Final telah dilatih, tetapi tidak ada data user yang valid untuk contoh rekomendasi.\")\n",
        "else:\n",
        "    print(\"\\nModel V2 Final belum dilatih atau tidak tersedia, contoh rekomendasi tidak dapat dijalankan.\")\n",
        "\n",
        "# Contoh 3: Pengguna baru hanya preferensi kategori (fallback sederhana)\n",
        "# Ini tidak memerlukan model NCF secara aktif, jadi bisa dijalankan\n",
        "new_user_prefs_example_2 = ['teknologi', 'sains'] # Gunakan kategori dari EXISTING_LIST_KATEGORI_WISATA\n",
        "print(f\"\\n>>> SKENARIO 3 (V2 Final): Rekomendasi untuk pengguna baru, preferensi: {new_user_prefs_example_2} (filter kategori sederhana)\")\n",
        "recs_new_simple_v2 = get_unified_recommendations(\n",
        "    preferred_categories=new_user_prefs_example_2,\n",
        "    n=3,\n",
        "    ncf_model=best_model_tuned_v2 if 'best_model_tuned_v2' in locals() and best_model_tuned_v2 is not None else Model(), # Berikan model dummy jika tidak ada\n",
        "    user_encoder_passed=user_encoder,\n",
        "    item_encoder_passed=item_encoder,\n",
        "    df_user_cleaned_passed=df_user_cleaned, # Diperlukan untuk konsistensi argumen\n",
        "    df_wisata_features_passed=df_wisata_features, # Diperlukan untuk konsistensi argumen\n",
        "    feature_columns_ncf_passed=feature_columns, # Diperlukan untuk konsistensi argumen\n",
        "    cosine_sim_matrix_passed=cosine_sim_matrix, # Diperlukan untuk konsistensi argumen\n",
        "    df_wisata_source_passed=df_wisata,\n",
        "    list_kategori_wisata_valid_passed=EXISTING_LIST_KATEGORI_WISATA\n",
        ")\n",
        "\n",
        "\n",
        "# Save the best model\n",
        "if 'best_model_tuned_v2' in locals() and best_model_tuned_v2 is not None:\n",
        "    best_model_tuned_v2.save(\"best_hybrid_ncf_model.h5\")\n",
        "    print(\"Model saved as 'best_hybrid_ncf_model.h5'\")\n",
        "\n",
        "    # Save the encoders and other necessary data\n",
        "    import pickle\n",
        "\n",
        "    model_data = {\n",
        "        'user_encoder': user_encoder,\n",
        "        'item_encoder': item_encoder,\n",
        "        'feature_columns': feature_columns,\n",
        "        'df_wisata': df_wisata,\n",
        "        'df_wisata_features': df_wisata_features,\n",
        "        'df_user_cleaned': df_user_cleaned,\n",
        "        'cosine_sim_matrix': cosine_sim_matrix,\n",
        "        'EXISTING_LIST_KATEGORI_WISATA': EXISTING_LIST_KATEGORI_WISATA\n",
        "    }\n",
        "\n",
        "    with open('model_data.pkl', 'wb') as f:\n",
        "        pickle.dump(model_data, f)\n",
        "    print(\"Auxiliary model data saved as 'model_data.pkl'\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "lLML8uU_oM5N",
        "outputId": "5d52b3e3-aa54-4c4e-bd39-d691a0d1bd1d"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Trial 15 Complete [00h 01m 03s]\n",
            "val_mae: 0.16356784105300903\n",
            "\n",
            "Best val_mae So Far: 0.12522290647029877\n",
            "Total elapsed time: 00h 29m 47s\n",
            "\n",
            "        Hyperparameter terbaik (V2 Final) yang ditemukan:\n",
            "        Embedding Dim: 96\n",
            "        MLP Unit 1: 48\n",
            "        MLP Unit 2: 16\n",
            "        MLP Unit 3: 8\n",
            "        Feature MLP Unit 1: 48\n",
            "        Feature MLP Unit 2: 16\n",
            "        Dropout Rate: 0.350\n",
            "        Learning Rate: 0.001\n",
            "        L2 Reg Factor: 0.0001\n",
            "        \n",
            "\n",
            "Melatih model terbaik (V2 Final) dengan hyperparameter yang ditemukan...\n",
            "Epoch 1/120\n",
            "\u001b[1m511/511\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m16s\u001b[0m 17ms/step - loss: 0.2777 - mae: 0.3515 - val_loss: 0.0960 - val_mae: 0.1705\n",
            "Epoch 2/120\n",
            "\u001b[1m511/511\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 5ms/step - loss: 0.0896 - mae: 0.1589 - val_loss: 0.0956 - val_mae: 0.1511\n",
            "Epoch 3/120\n",
            "\u001b[1m511/511\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 5ms/step - loss: 0.0858 - mae: 0.1410 - val_loss: 0.1038 - val_mae: 0.1442\n",
            "Epoch 4/120\n",
            "\u001b[1m511/511\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 5ms/step - loss: 0.0731 - mae: 0.1067 - val_loss: 0.0940 - val_mae: 0.1439\n",
            "Epoch 5/120\n",
            "\u001b[1m511/511\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 5ms/step - loss: 0.0608 - mae: 0.0966 - val_loss: 0.0906 - val_mae: 0.1422\n",
            "Epoch 6/120\n",
            "\u001b[1m511/511\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 5ms/step - loss: 0.0532 - mae: 0.0894 - val_loss: 0.0891 - val_mae: 0.1355\n",
            "Epoch 7/120\n",
            "\u001b[1m511/511\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 5ms/step - loss: 0.0443 - mae: 0.0773 - val_loss: 0.0869 - val_mae: 0.1322\n",
            "Epoch 8/120\n",
            "\u001b[1m511/511\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 6ms/step - loss: 0.0408 - mae: 0.0715 - val_loss: 0.0852 - val_mae: 0.1331\n",
            "Epoch 9/120\n",
            "\u001b[1m511/511\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 5ms/step - loss: 0.0398 - mae: 0.0702 - val_loss: 0.0839 - val_mae: 0.1322\n",
            "Epoch 10/120\n",
            "\u001b[1m511/511\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 5ms/step - loss: 0.0392 - mae: 0.0676 - val_loss: 0.0854 - val_mae: 0.1301\n",
            "Epoch 11/120\n",
            "\u001b[1m511/511\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 5ms/step - loss: 0.0369 - mae: 0.0637 - val_loss: 0.0848 - val_mae: 0.1310\n",
            "Epoch 12/120\n",
            "\u001b[1m511/511\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 5ms/step - loss: 0.0361 - mae: 0.0618 - val_loss: 0.0845 - val_mae: 0.1310\n",
            "Epoch 13/120\n",
            "\u001b[1m511/511\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 5ms/step - loss: 0.0353 - mae: 0.0596 - val_loss: 0.0832 - val_mae: 0.1303\n",
            "Epoch 14/120\n",
            "\u001b[1m511/511\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 5ms/step - loss: 0.0335 - mae: 0.0579 - val_loss: 0.0843 - val_mae: 0.1302\n",
            "Epoch 15/120\n",
            "\u001b[1m511/511\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 5ms/step - loss: 0.0336 - mae: 0.0555 - val_loss: 0.0852 - val_mae: 0.1290\n",
            "Epoch 16/120\n",
            "\u001b[1m511/511\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 5ms/step - loss: 0.0342 - mae: 0.0556 - val_loss: 0.0821 - val_mae: 0.1298\n",
            "Epoch 17/120\n",
            "\u001b[1m511/511\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 5ms/step - loss: 0.0324 - mae: 0.0542 - val_loss: 0.0845 - val_mae: 0.1282\n",
            "Epoch 18/120\n",
            "\u001b[1m511/511\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 5ms/step - loss: 0.0311 - mae: 0.0523 - val_loss: 0.0847 - val_mae: 0.1281\n",
            "Epoch 19/120\n",
            "\u001b[1m511/511\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m6s\u001b[0m 6ms/step - loss: 0.0310 - mae: 0.0520 - val_loss: 0.0831 - val_mae: 0.1273\n",
            "Epoch 20/120\n",
            "\u001b[1m511/511\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 5ms/step - loss: 0.0302 - mae: 0.0501 - val_loss: 0.0831 - val_mae: 0.1275\n",
            "Epoch 21/120\n",
            "\u001b[1m511/511\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 5ms/step - loss: 0.0292 - mae: 0.0494 - val_loss: 0.0821 - val_mae: 0.1260\n",
            "Epoch 22/120\n",
            "\u001b[1m511/511\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 5ms/step - loss: 0.0287 - mae: 0.0490 - val_loss: 0.0832 - val_mae: 0.1268\n",
            "Epoch 23/120\n",
            "\u001b[1m511/511\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 5ms/step - loss: 0.0292 - mae: 0.0479 - val_loss: 0.0822 - val_mae: 0.1275\n",
            "Epoch 24/120\n",
            "\u001b[1m511/511\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 5ms/step - loss: 0.0283 - mae: 0.0471 - val_loss: 0.0826 - val_mae: 0.1277\n",
            "Epoch 25/120\n",
            "\u001b[1m511/511\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 5ms/step - loss: 0.0279 - mae: 0.0466 - val_loss: 0.0799 - val_mae: 0.1289\n",
            "Epoch 26/120\n",
            "\u001b[1m511/511\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 5ms/step - loss: 0.0275 - mae: 0.0474 - val_loss: 0.0813 - val_mae: 0.1276\n",
            "Epoch 27/120\n",
            "\u001b[1m511/511\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 6ms/step - loss: 0.0267 - mae: 0.0461 - val_loss: 0.0797 - val_mae: 0.1265\n",
            "Epoch 28/120\n",
            "\u001b[1m511/511\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 5ms/step - loss: 0.0259 - mae: 0.0452 - val_loss: 0.0811 - val_mae: 0.1262\n",
            "Epoch 29/120\n",
            "\u001b[1m511/511\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 5ms/step - loss: 0.0276 - mae: 0.0465 - val_loss: 0.0824 - val_mae: 0.1259\n",
            "Epoch 30/120\n",
            "\u001b[1m511/511\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 5ms/step - loss: 0.0269 - mae: 0.0456 - val_loss: 0.0816 - val_mae: 0.1276\n",
            "Epoch 31/120\n",
            "\u001b[1m511/511\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 5ms/step - loss: 0.0267 - mae: 0.0467 - val_loss: 0.0792 - val_mae: 0.1278\n",
            "Epoch 32/120\n",
            "\u001b[1m511/511\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 6ms/step - loss: 0.0257 - mae: 0.0447 - val_loss: 0.0792 - val_mae: 0.1255\n",
            "Epoch 33/120\n",
            "\u001b[1m511/511\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 5ms/step - loss: 0.0244 - mae: 0.0435 - val_loss: 0.0799 - val_mae: 0.1264\n",
            "Epoch 34/120\n",
            "\u001b[1m511/511\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 5ms/step - loss: 0.0256 - mae: 0.0450 - val_loss: 0.0788 - val_mae: 0.1275\n",
            "Epoch 35/120\n",
            "\u001b[1m511/511\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 5ms/step - loss: 0.0254 - mae: 0.0461 - val_loss: 0.0779 - val_mae: 0.1266\n",
            "Epoch 36/120\n",
            "\u001b[1m511/511\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 5ms/step - loss: 0.0250 - mae: 0.0456 - val_loss: 0.0792 - val_mae: 0.1266\n",
            "Epoch 37/120\n",
            "\u001b[1m511/511\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 5ms/step - loss: 0.0243 - mae: 0.0453 - val_loss: 0.0779 - val_mae: 0.1280\n",
            "Epoch 38/120\n",
            "\u001b[1m511/511\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 5ms/step - loss: 0.0244 - mae: 0.0451 - val_loss: 0.0781 - val_mae: 0.1260\n",
            "Epoch 39/120\n",
            "\u001b[1m511/511\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 5ms/step - loss: 0.0239 - mae: 0.0436 - val_loss: 0.0780 - val_mae: 0.1267\n",
            "Epoch 40/120\n",
            "\u001b[1m511/511\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 5ms/step - loss: 0.0228 - mae: 0.0434 - val_loss: 0.0778 - val_mae: 0.1264\n",
            "Epoch 41/120\n",
            "\u001b[1m511/511\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 5ms/step - loss: 0.0233 - mae: 0.0446 - val_loss: 0.0786 - val_mae: 0.1270\n",
            "Epoch 42/120\n",
            "\u001b[1m511/511\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 5ms/step - loss: 0.0234 - mae: 0.0447 - val_loss: 0.0770 - val_mae: 0.1266\n",
            "Epoch 43/120\n",
            "\u001b[1m511/511\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 5ms/step - loss: 0.0222 - mae: 0.0432 - val_loss: 0.0768 - val_mae: 0.1257\n",
            "Epoch 44/120\n",
            "\u001b[1m511/511\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 5ms/step - loss: 0.0221 - mae: 0.0427 - val_loss: 0.0761 - val_mae: 0.1265\n",
            "Epoch 45/120\n",
            "\u001b[1m511/511\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 6ms/step - loss: 0.0220 - mae: 0.0432 - val_loss: 0.0765 - val_mae: 0.1273\n",
            "Epoch 46/120\n",
            "\u001b[1m511/511\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 5ms/step - loss: 0.0224 - mae: 0.0438 - val_loss: 0.0760 - val_mae: 0.1268\n",
            "Epoch 47/120\n",
            "\u001b[1m511/511\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 5ms/step - loss: 0.0219 - mae: 0.0443 - val_loss: 0.0758 - val_mae: 0.1273\n",
            "Epoch 47: early stopping\n",
            "Restoring model weights from the end of the best epoch: 32.\n",
            "\n",
            "Model Tuned (V2 Final) - Test Loss: 0.0792\n",
            "Model Tuned (V2 Final) - Test MAE: 0.1255\n",
            "\n",
            ">>> SKENARIO 1 (V2 Final): Rekomendasi untuk pengguna dikenal 'U000001'\n",
            "Membuat rekomendasi untuk pengguna yang sudah dikenal: U000001\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "<ipython-input-5-c93895899587>:461: SettingWithCopyWarning: \n",
            "A value is trying to be set on a copy of a slice from a DataFrame.\n",
            "Try using .loc[row_indexer,col_indexer] = value instead\n",
            "\n",
            "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
            "  top_n_final['ID Tempat'] = top_n_final['item_id_int'].map(item_id_int_to_raw_map)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Top 5 Rekomendasi Gabungan (NCF + Cosine Boost) untuk User 'U000001':\n",
            "  ID Tempat              Nama Wisata  ncf_score_norm  content_affinity_score  combined_score %\n",
            "0      T002     Kampung Batik Kauman        0.997911                0.752798         92.437701\n",
            "1      T004  Museum Batik Danar Hadi        0.997249                0.639852         89.002947\n",
            "2      T020       Pura Mangkunagaran        0.999615                0.053349         71.573502\n",
            "3      T081      Girimanik Waterfall        0.997724                0.052146         71.405032\n",
            "4      T049        Kemuning Skyhills        0.997070                0.051111         71.328206\n",
            "\n",
            ">>> SKENARIO 2 (V2 Final): Rekomendasi untuk pengguna baru, preferensi: ['budaya', 'seni'] (dengan NCF Probe)\n",
            "Probe User IDs: ['U001505', 'U015015', 'U034974']\n",
            "Membuat rekomendasi untuk pengguna baru dengan preferensi: ['budaya', 'seni'] (NCF probe)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "WARNING:absl:You are saving your model as an HDF5 file via `model.save()` or `keras.saving.save_model(model)`. This file format is considered legacy. We recommend using instead the native Keras format, e.g. `model.save('my_model.keras')` or `keras.saving.save_model(model, 'my_model.keras')`. \n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Top 5 Rekomendasi Hybrid (Kategori + NCF Probe) untuk Pengguna Baru:\n",
            "   ID Tempat                   Nama Wisata  Overall Rating (Google Maps)  google_rating_norm  avg_ncf_appeal_score  final_score %  budaya  seni\n",
            "23      T024         Museum Astana Oentara                           4.9               1.000              0.961869      98.093441       1     0\n",
            "10      T011        Tumurun Private Museum                           4.8               0.875              0.998430      93.671492       1     1\n",
            "20      T021              Museum Lokananta                           4.8               0.875              0.973669      92.433453       1     1\n",
            "90      T091  Masjid Agung Al-Aqsha Klaten                           4.8               0.875              0.952536      91.376781       0     1\n",
            "19      T020            Pura Mangkunagaran                           4.7               0.750              0.993062      87.153092       1     0\n",
            "\n",
            ">>> SKENARIO 3 (V2 Final): Rekomendasi untuk pengguna baru, preferensi: ['teknologi', 'sains'] (filter kategori sederhana)\n",
            "Membuat rekomendasi untuk pengguna baru HANYA berdasarkan kategori: ['teknologi', 'sains'] (tanpa NCF probe).\n",
            "\n",
            "Top 3 Rekomendasi Berdasarkan Kategori (Simple Filter):\n",
            "   ID Tempat                        Nama Wisata  Overall Rating (Google Maps)  Jumlah Ulasan (Google Maps)  teknologi  sains\n",
            "66      T067        Sadeyan Desa Wisata Edukasi                           5.0                            2          1      1\n",
            "46      T047  Agrowisata Strawberry Tawangmangu                           4.7                          163          0      1\n",
            "6       T007                      De Tjolomadoe                           4.6                        21515          1      0\n",
            "Model saved as 'best_hybrid_ncf_model.h5'\n",
            "Auxiliary model data saved as 'model_data.pkl'\n"
          ]
        }
      ]
    }
  ]
}